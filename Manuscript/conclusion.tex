\section{Conclusions}\label{Conclusions}

In this dissertation, the author proposed a portable load balancer with ECMP redundancy for the Kubernetes cluster systems, which is aimed at facilitating the migration of container clusters for web applications.
The proposed load balancer architecture utilizes software load balancers with container technology to make the load balancers runnable in any base infrastructure.
It also utilizes ECMP technology to make multiple load balancers active, and thereby to provide redundancy and scalability.


The author implemented a containerized software load balancer that is run by Kubernetes as a part of container cluster, using Linux kernel's ipvs.
In order to discuss the feasibility of the proposed load balancer, performance measurements are conducted in the 1 Gbps network environment.
It was shown that the proposed load balancers are runnable in an on-premise data center, GCP and AWS.
Therefore the proposed load balancers can be said to be portable.
The throughput levels of a load balancer are dependent on settings for multi-core packet processing.
It was shown that better to use as many CPU cores as possible for packet processing.
The throughput levels are also very dependent on the overlay network backend mode.
The host-gw mode where no tunneling is used resulted in the best performance level, and the vxlan mode resulted in the second best.
In the experiment in the 1 Gbps network environment, the ipvs-nat load balancer in the container had the same performance level as the internal load balancer using iptables DNAT provided by Kubernetes.
Furthermore, the performance level of ipvs-tun(one of the operation modes of ipvs) load balancer in a container with the L3DSR setup was about 1.5 times larger than that of iptables DNAT.
Therefore in 1 Gbps network environment, the proposed load balancer is portable while it has the 1.5 times better performance level than internal load balancer provided by Kubernetes.

Also implemented is the ECMP setups where multiple of the load balancer containers are deployed, each advertising the route to the IP for the web application through BGP.
The ECMP technique makes the load balancers redundant and scalable since all the load balancer containers act as active.
The whole system is resilient to a single failure of load balancer container.
Also by utilizing multiple load balancers simultaneously, the throughput of the total system is increased significantly.
These characteristics are evaluated by checking the routing table of the upstream router and by throughput measurement.

%
The author verified that ECMP routing table was properly created in the experimental system.
The update of the ECMP routing table was correct and quick enough, i.e., within 10 seconds, throughout 20 hours experiment.
The maximum performance levels of the cluster of load balancers scaled linearly as the number of the load balancer pods was increased up to four of them.
The maximum throughput level obtained through the experiment was 780k [req/sec], which is limited due to the maximum CPU performance of the benchmark client rather than the performance of the load balancer cluster.

The author also extended the throughput measurement into the 10 Gbps network environment.
It was revealed that ipvs and ipvs-tun load balancers in containers had lower performance levels compared with the iptables DNAT.
The reason for this has been verified to be due to the overhead of the container network, i.e., veth+bridge and inefficiency of the ipvs itself.
The author also implemented a novel software load balancer using XDP technology to enhance the performance of software load balancer and presented preliminary performance result.
The current implementation does not support multicore packet processing, and hence throughput is limited by the capability of single core processing performance.
However, the obtained throughput about 390K [req/sec] for the XDP load balancer(xlb) is nearly the half of the iptables DNAT with eight physical core packet processing, which the author considers very promising.
 
The outcome of this study will benefit users who want to deploy their web services on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of this study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web service on aggregated computing resources on the earth, as if they were starting a single process on a single computer.



 
