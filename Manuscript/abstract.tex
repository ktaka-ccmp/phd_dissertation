\chapter*{Abstract}

Today, most of the people in the world can not spend a day without smartphones or PCs.
They use those devices to access services provided by web applications on the Internet.
These services included e-mail, social media, search engines, shopping site etc., everything provided through the Internet.
As the services become indispensable part of the daily lives, operating web applications stably and swiftly becomes important day by day.
For example, those who provide these services need to be able to recover from a disaster, or start their web shopping site in other countries,
by migrating their web applications to different locations swiftly and safely.
For such purposes, providing a web application using a cluster of Linux containers is a promising candidate, since Linux containers can be run on any Linux system regardless of the infrastructures.


A container orchestrator (also called container cluster management system) is a tool to simplify the management of a cluster of containers that are launched on multiple servers.
And it is expected to provide a uniform platform for container clusters, which also facilitates the migration of web applications.
However, none of the existing container orchestrators fully supports an automatic setup of ingress traffic routing from the Internet.
This is one of the most critical problems for container orchestrators because without solving this problem, the migration of a web application will never be easy.

In this dissertation, the author addresses this problem by providing a portable software load balancer that is capable of an automatic setup of the ingress traffic routing.
The author proposes a portable load balancer for the Kubernetes cluster systems, that is aimed at facilitating the migration of container clusters for web applications.
The proposed load balancer architecture utilizes software load balancers with container technology to make the load balancers runnable in any base infrastructure. 
It also supports an automatic setup of ECMP routes to make multiple load balancers active, and thereby to provide redundancy and scalability.

The author implemented a containerized software load balancer that is run by Kubernetes using Linux kernel's ipvs.
A cluster of load balancers in containers is launched with web application clusters regardless of the base infrastructure.
In order to discuss the feasibility of the proposed load balancer, performance measurements have been conducted in the 1 Gbps network environment.
It has been shown that the proposed load balancers are runnable in an on-premise data center, GCP and AWS.
Therefore the proposed load balancers can be said to be portable.
%
The throughputs of a load balancer are dependent on settings for multi-core packet processing.
It has been shown that the setting using as many CPU cores as possible for packet processing results in better performance.
The throughputs are also very dependent on settings for the overlay network, where the backend mode without any packet encapsulation has resulted in the best performance.
%
Furthermore, the throughput for the (Layer 3 Direct Server Return)L3DSR setting of the ipvs has been about 1.5 times larger than that of iptables DNAT.
Therefore in 1 Gbps network environment, the proposed load balancer is portable while it has the 1.5 times better performance level than internal load balancer provided by Kubernetes.

The author also has implemented an automatic setup of the ECMP route for ingress traffic.
There, multiple load balancer containers are deployed, and each of them advertises itself as an active next hop of the IP for web application through BGP.
The ECMP route makes the load balancers redundant and scalable since all the load balancer containers act as active.
The BGP helps automatic setup of the ECMP route.  

%% The whole system is resilient to a single failure of load balancer container.
%% Also by utilizing multiple load balancers simultaneously, the throughput of the total system is increased significantly.
%% These characteristics are evaluated by checking the routing table of the upstream router and by throughput measurement.

%
The author verified that an ECMP route has been automatically created upon launch of a new load balancer container.
The update of the ECMP routing table was correct and quick enough, i.e., within 10 seconds, throughout 20 hours experiment.
The maximum performance levels of the cluster of load balancers have scaled linearly as the number of the load balancer containers has been increased up to four of them.
The maximum throughput obtained through the experiment is 780k [req/sec], which is limited by the CPU performance of the benchmark client, and therefore can be improved using better hardware in the future experiment.

The author also extended the throughput measurement into the 10 Gbps network environment.
It has been revealed that ipvs and ipvs-tun load balancers in containers have lower performance levels compared with the iptables DNAT.
This is due to the overhead of the container network and inefficiency of ipvs itself.

The author also implemented a novel software load balancer using XDP technology to further enhance the performance of software load balancer and presented preliminary performance result.
The current implementation does not support multicore packet processing, and hence throughput is limited by the capability of single core processing performance.
However, the obtained throughput about 390K [req/sec] for the XDP load balancer(xlb) is nearly the half of the iptables DNAT with eight physical core packet processing, which the author considers very promising.
 
The outcome of this study will benefit users who want to deploy their web services on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of this study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web service on aggregated computing resources on the earth, as if they were starting a single process on a single computer.


