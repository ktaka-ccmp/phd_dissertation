%\section{Introduction}

\section{Motivation}

\subsection{Web application cluster}

Today, a great number of people in the world can not spend a day without using smartphones or personal computers(PCs) to retrieve information from the Internet for work or for daily life.
For example, people use these devices to look up web pages, emails, social media and sometimes to play games.
These services are often called web applications or web services, where information is delivered using Hyper Text Transfer Protocols(HTTP) or Hypertext Transfer Protocol Secure (HTTPS) from servers at the other end of the Internet.
Web applications are provided by various organizations, including commercial companies, government, non-profitable organizations, etc.

For example, Google provides a variety of web services including, Gmail, Search engine, Google Suits, etc.
Facebook provides social media service, Amazon provides shopping sites.
Governments provides information regarding the service they provide to their citizens.
Schools provide a syllabus to their students.
The author calls those organizations that provide web applications, web application providers hereafter.

A client program on PCs or smartphone sends out requests to servers and the servers respond with data that is requested, using HTTP or HTTPS. 
Servers for web applications are usually computers located in a data center.
%Servers also refer to the server programs that are runing on these computers. 
In the data center multiple servers cooperate to fulfill the need of the clients.
A group of these servers is often called a web application cluster or a web cluster.
Figure~\ref{fig:web_cluster} shows schematic diagram of an example of a web cluster.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\columnwidth]{Figs/web_cluster.png}
\end{center}
\caption{
An example of web cluster.
}
\centering\parbox[c]{0.9\columnwidth}{
The load balancers distribute requests from clients to multiple web servers.
The web servers form responses using data retrieved from the database servers and send it back to the clients.
Sometimes the web servers also store and update important data into the database servers.
}
\label{fig:web_cluster}
\end{figure}

In this example, there are two load balancers, four web servers and two Database(DB) servers that work together to respond to requests from clients.
The load balancers distribute requests from clients to multiple web servers.
Then the web servers form responses using data retrieved from the database servers and send it back to the clients.
Sometimes the web servers also store and update important data into the database servers.

\subsection{Web cluster migration}

As web applications become an essential part of daily life, an outage of the service is getting to be a bigger problem.
People will not be able to access the necessary information once something happens to a web cluster in a data center.

For example, if web pages run by local government stops, people will not be able to access the information regarding public service.
If a shopping site run by a company stops, customers cannot buy products and the revenue of the company will be negatively affected.
Outages of web applications by giant companies can have an even bigger impact.
An outage of Gmail or Google search engine will stop most of the business activities around the world.
Service down of Amazon.com affect buyers and businesses that sell products on its platform.

In order to prevent such outages, preparing another web application cluster in a different location in the case for disasters is very important.
For that purpose, it is desirable if a web application cluster can be easily migrated to a different data center.
Migration of a web cluster becomes more realistic with the use of container technology, which is explained later.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\columnwidth]{Figs/earth}
\end{center}
\caption{
Migration of web application cluster to different locations.
}
\centering\parbox[c]{0.9\columnwidth}{
It is desirable to be able to migrate a web cluster from one place to another with the easiness of on push button.
}
\label{fig:earth}
\end{figure}

Migration capability of a web cluster also has other benefits.
If an e-commerce service is successful in Japan, the company that runs the service might want to start the same service in other country, let's say, in Europe.
In this situation, the company probably wants to migrate its web applictation cluster to somewhere in Europe, because, for Europian customers, responses from a web site in Europe is quicker than that in Japan.

Being able to migrate a web application cluster is very important for a variety of purposes, including disaster recovery, cost performance optimizations, meeting legal compliance and shortening the geographical distance to customers.
These are the main concerns for web application providers in e-commerce, gaming, Financial technology(Fintech) and Internet of Things(IoT) field.
Therefore it is important for a web application provider to be able to easily deploy and migrate their web applications among different infrastructure around the world.
The purpose of this research is to propose infrastructures for web application providers, where they can easily deploy their services across the world, regardless of cloud providers or data centers they use.


\subsection{Ideal infrastructure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Figs/global_container_infrastructure}
\end{center}
\caption{
An ideal global container infrastructure.
}
\centering\parbox[c]{0.9\columnwidth}{
Web clusters consisting of a cluster of containers are deployed in three different data centers as an example.
In each of the data center, container orchestrator manages a web cluster. 
Important data are stored in a globally consistent data store.  
Access from the client is routed to the closest data center using anycast.
}
\label{fig:global_container_infrastructure}
\end{figure}

In order to realize an easy migration of web applications, an ideal infrastructure probably have the following features;
1) have universal middleware to manage web clusters,
2) store data in globally consistent data storage,
3) route global traffic based on proximity to the client.

Figure\ref{fig:global_container_infrastructure} shows an exemplified global container infrastructure having these features.
A container orchestrators launch and manage container.
Important data are stored in globally consistent data storage, which is simillar to the Google spanner\cite{Corbett:2013:SGG:2518037.2491245,Cooper:2013:SGG:2485732.2485756} or CockroachDB\cite{pavlo2016s}.
Traffic is routed to the closest data center using anycast\cite{rfc1546}.

Each of these features is important and research efforts are on-going in many institutions.
In this study, the author focuses on the research regarding container orchestrator as a universal middleware.

By realizing global container infrastructure web application providers will be able to deploy their service whenever and wherever they want.
Also, they will be able to move their services quickly depending on a variety of circumstances, including disaster recovery, cost performance optimization and compliance to government regulations due to trade wars.


\section{Inftastructure for web applications}

\subsection{On-premise data center}

Historically, most of the Web application providers purchased servers and installed them in server housing facilities called data centers.
In this type of infrastructure, web application providers typically need to sign a contract with data center company for server housing racks, buy servers and install them in their rented racks by themselves.
They also install OS and software stacks needed to run their web applications in the servers.
Since web application providers place servers in their (either owned or rented) facilities, and they are responsible for managing the servers, this type of infrastructure is often called on-premise infrastructure in contrast to Cloud Computing infrastructure.

Preparing data centers, installing the servers and configuring software stacks for web application services often require a considerable amount of time, money and effort.
If web application providers want to expand their services to different countries or if they want to prepare for natural disasters by preparing an additional web cluster in a different data center, they most likely need about the same amount of time, money and effort required to build their original infrastructures.
Therefore migration web application in this type of infrastructures has always been a daunting task.

\subsection{Cloud computing}

The emergence of Cloud Computing made many things easier for web application providers than before.
Cloud computing utilizes a virtual machine(VM) technology, e.g. KVM, Xen, and VMware.
Cloud computing service providers offer VMs to web application providers(users) with pay-per-use billing.

Figure~\ref{fig:physical_vm_container} compares different type of usages of a single physical server and Figure~\ref{fig:physical_vm_container} (b) shows an example architecture of VM technology.
VMs share a single physical server.
A full OS including Linux kernel is running on top of the virtual machine represented by the hypervisor.
Each VM behaves almost as same as a single physical server.
Since VMs are fractions of a single physical server, server resources are utilized with finer granularities.
Web application providers can start their services with a cluster of VMs, which is smaller than a cluster of physical servers, and hence resulting in lower cost.
Cloud providers generally prepare physical servers and software stacks for VMs before renting it to users, and they also provide an easy to use web user interfaces.
As a result, users only need to click a few buttons on web browsers and wait for a few minutes in order to obtain up-and-running VMs.
This simplicity will bring agility to web application providers when they launch their services.
And since computing resources are offered with per-second pay-per-use billing, web application providers can quickly reduce the cost by stopping excessive VMs, when the demand for computing power decreases.
This was impossible when web application providers purchased physical servers and installed them in a conventional on-premise data center.
In short cloud computing brought the users agility, flexibility, and cost-effectiveness.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\columnwidth]{Figs/physical_vm_container.png}
\end{center}
\caption{
The difference in physical server usage between (a) Bare Metal servers, (b) Virtual Machine and (c) Container technology.
}

\centering\parbox[c]{0.9\columnwidth}{
(a) Bare Metal servers is a word to describe conventional physical servers in contrast to Virtual Machines.
On top of a Bare Metal server, an operating system and application programs are running.
(b) Virtual Machine technology utilizes physical server hardware and a hypervisor.
The hypervisor provides generic representations of server hardware, which are called virtual machines.
A full operating system and applications are running on each of the virtual machines.
(c) Container technology separates applications by containing them to their respective namespaces.
Applications can not see each other's file systems, networks, users and process IDs unless they belong to the same namespace.
Since container technology merely relies on Linux kernel's namespace function and optionally cgroup, a containerized process does not have any additional overhead compared with a process running on a conventional physical server and operating system.
Container technology can be also utilized on top of virtual machines.
}
\label{fig:physical_vm_container}
\end{figure}

\subsection{Container technology}

More recently, Linux containers\cite{menage2007adding} have come to draw a significant amount of attention.
Figure~\ref{fig:physical_vm_container} (c) shows an example architecture of container technology. 
Linux containers are the processes that have separate execution environments that are created using the Linux kernel's namespace feature.
The namespace feature can isolate visibility of resources on a single Linux server.

Every process in a container is assigned to a certain namespace, and if two processes belong to different namespaces, they can not see each other's resources. 
Linux kernel implements filesystem namespace, PID namespace, network namespace, user namespace, IPC namespace, and hostname namespace. 
For example, every filesystem namespace can have its own root filesystem, and every network namespace can have its own network devices and IP addresses.
Therefore, it is possible to configure processes as if they were running in different Linux systems by assigning them to different namespaces, although they share kernel and hardware.
While each VM needs to run a full OS on top of a hypervisor and hence imposes extra overhead, a process in Linux container is merely a process with a dedicated namespace and hence imposes no extra overhead.

The Linux container can run on any Linux systems including physical servers and VM servers.
Due to the widespread usage of Linux systems, the Linux container can run in most of the cloud infrastructures and on-premise data centers.

Several management tools are available for Linux containers, including LXC\cite{noronha2018performance}, systemd-nspawn\cite{jedge2013} and Docker\cite{merkel2014docker}.
These tools assign an appropriate namespace to a process upon the launch of itself and make it look like running in its own virtual Linux system.
For example, container tools restore a file system from an archive file every time a container is launched. 
Container tools also set up separate network interfaces with separate IP addresses in the container's namespace.

The fact that each container has its own file system that is restored from a single archive file brings a significant benefit, i.e. a program binary and shared libraries are always exactly the same regardless of the base infrastructure.
Therefore a process in a container is guaranteed to behave exactly the same manner, even if totally different data centers or cloud providers are used.
This was not easy when there was no container technology.
Because there are many flavors of Linux distributions, and even if the same distribution is used, there was always a chance that a slight difference in a program binary version or library versions could have broken the expected behavior.

In addition to that, containers on a single server can have own version of libraries on their respective filesystems, in other words, libraries in any container can be independently updated without influencing other containers.
In conventional technologies, processes on a single server are dependent on common shared libraries, and hence updates of the library sometimes have caused unexpected side effects.

Thanks to these benefits container technologies are very attractive for web applications.
Considerable efforts in utilizing container technologies for web applications are ongoing.
And to facilitate deployment of a web application that consists of a cluster of containers, several container orchestrator(container management systems) have been in development.

\subsection{Container Orchestrator}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Figs/container_management_system}
\end{center}
\caption{
A web application cluster and container orchestrator.
}
\centering\parbox[c]{0.9\columnwidth}{
A web application cluster consists of nginx(http), redis(key-value store) and mysqld(database) is depicted in this figure. 
Each of the component consists of a cluster of containers.
Container orchestrator recieve configuration file(1), schdule containers(2), set up ingress routing using load balancer(3) and set up internal routing(4).
}
\label{fig:container_management_system}
\end{figure}

A container orchestrator (also called container cluster management system) is a tool to simplify the management of a cluster of containers that are launched on multiple servers.
A container orchestrator is the most important part of the infrastructure for easy migration, not only because it can act as common middleware that hides differences among base infrastructures, but also because it can drastically simplify the deployment of interdependent container clusters.
Figure~\ref{fig:container_management_system} shows the important features for the container orchestrator for web applications;
1) {\bf Configuration file:} Orchestrator manages container clusters based on a configuration file. 
The configuration file must have descriptions of container cluster internals, and relationships between multiple clusters.
2) {\bf Scheduling:} Depending on the configuration file, the orchestrator must pick servers and launch containers on them.
Orchestrator must also maintain the state described in configuration files, for example, the orchestrator may need to maintain the number of running containers.
3) {\bf Ingress routing:} The orchestrator must be able to set up routes for incoming traffic from the internet to a group of containers in a redundant and scalable manner.
4) {\bf Internal routing:} If the web application consists of multiple groups of containers, the orchestrator must be able to set up routes between the groups in a redundant and scalable manner.

In a configuration file a user can describe how a cluster of containers should be configured, and also can describe relationships between different clusters.
As a result, a user can launch a web application that consists of interdependent clusters of containers just by supplying the configuration file to the orchestrator.
For example, a web cluster in Figure~\ref{fig:container_management_system} consists of three different functionalities, namely http server, key-value store, and database.
Each of those consists of multiple containers.
In the configuration file, the relationships between http server cluster, key-value store cluster, and database cluster are specified.
The configuration file also contains how each cluster should be run, including the number of the containers and resources assigned to them.
Users only need to feed configuration files to the orchestrator to launch complex web applications.

Thanks to these features, an orchestrator can be viewed as if it is an Operating System for a server farm in a data center, which not only schedules and launches processes on the server farm but also routes the traffic to the right processes.
By using orchestrators, a user can start a web application that consists of a cluster of containers, on multiple servers in a data center, as easily as starting a single process on a single computer.
As a consequence, a user can easily migrate their web applications at his or her convenience.

Several container orchestrators are available, including Kubernetes, Docker swarm and Mesos/Marathon.
Each of the Container Management System varies in target applications, and thus has the strength and weaknesses.

\paragraph{Kubernetes}
Kubernetes\cite{burns2016borg} is a open source container orchestrator, originally developed at Google based on their experience of production container orchestrator, Borg\cite{Verma2015}. 
Since Google runs many of large scale web applications, Kubernetes is best suited to run web applications.

\paragraph{Docker swarm}
Docker Swarm is a container orchestrator built in Docker daemon itself.
Users can execute regular Docker commands, which are then executed by a swarm manager. 
The swarm manager is responsible for controlling the deployment and the life cycle of containers.

\paragraph{Mesos/Marathon}
Mesos\cite{hindman2011mesos} is a common resource sharing layer for different type of applications like Hadoop, MPI jobs, and Spark in a Data Center. 
By using Mesos user does not neet to have dedicated pysical server cluster for each applications.
Marathon is a framework which uses Mesos in order to orchestrate Docker containers.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    & \multicolumn{1}{c|}{Kuberenets} & \multicolumn{1}{c|}{Docker Swarm} & \multicolumn{1}{c|}{Mesos Marathon} \\ \hline
    Config file & Yaml & Yaml & Json  \\ \hline
    Scheduling & Yes &  Yes &  Yes  \\ \hline
    Ingress routing & \begin{tabular}{c} Static\\ Cloud load balancer$^{**}$\end{tabular} & Static & Static \\ \hline
    Internal routing & iptables DNAT & ipvs &  haproxy  \\ \hline
  \end{tabular}
  \caption{Container orchestrator comparison.}
  \centering\parbox[c]{0.9\columnwidth}{
    $^{**}$The support for Cloud load balancer is only avalable in limited infrastructures including GCP, AWS, Azure and Openstack.
  }
  \label{table:orchestrator_comparison}
\end{table}

Table\ref{table:orchestrator_comparison} compares these orchestrators based on necessary features as an infrastructure for web applications.
All of these orchestrators rely on static routing for ingress traffic.
Although Kubernetes has the functionality to manage cloud load balancer so that ingress traffic from the Internet is routed in a redundant and scalable manner, this functionality is only applicable in a few cloud environments.

To the best knowledge of the author, none of the existing container orchestrators has full support for redundant and scalable ingress routing feature.
The author believes this is an open and important topic for research and development.

\section{Focus of this study}

\subsection{Load balancer for container clusters}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Figs/cluster_of_container_loadbalancer}
\end{center}
\caption{
Load balancer for container clusters.
}
\centering\parbox[c]{0.9\columnwidth}{
In order distribute the traffic, container orchestrator launches a cluster of software load balancer containers. 
The container orchestrator also communicates with the upstream router through BGP protocol and the router set up an ECMP routing rule in the routing table.
}
\label{fig:cluster_of_container_loadbalancer}
\end{figure}

The purpose of this research is to investigate a generic way to route the traffic into container clusters in a redundant and scalable manner and thereby to facilitate web application migrations.
In order to realize that, the author proposes a cluster of software load balancer containers.

Figure~\ref{fig:cluster_of_container_loadbalancer} shows schematic diagram of an example architecture for such load balancer cluster.
Since the orchestrator is good at scaling containers, i.e. change the number of containers depending on the needs,
it seems a good idea to deploy load balancers as a cluster of containers.
By utilizing Equal Cost Multi Path(ECMP) routing table, redundancy and scalability are accomplished at the same time.

The author investigates portable software load balancer for Kubernetes as a test case since Kubernetes seems most appropriate for web application clusters at the moment.
However, the author expects general findings regarding the portable and scalable load balancers can be easily applied to the other container orchestrators as well.

\subsection{Problems of Kubernetes}

It is desirable if users can migrate their services to multiple of cloud providers or on-premise data centers seamlessly, which spread across the world.
Container cluster management systems facilitate these usages by functioning as middlewares, which hide the differences among cloud providers and on-premise data centers.

Kubernetes\cite{K8s2017}, which is one of the most popular container cluster management systems, enables easy deployment of container clusters.
Kubernetes are initially developed by engineers inside Google, to facilitate container cluster deployment for web applications.
Kubernetes allows users to deploy a cluster of containers each of which depends on each other, with the ease of launching a single application program.
It also allows users to increase or decrease the number of containers dynamically depending on the amount of traffic that they have to respond.

Since Kubernetes is expected to hide the differences in the base environments, it is expected that users can easily deploy a web application on different cloud providers or on on-premise data centers, without adjusting the container cluster configurations to the new environment. 
This allows a user to easily migrate a web application consisting of a container cluster even to the other side of the world.
A typical web application migration scenario is; 
a user starts the container cluster in the new location, route the traffic there, then stop the old container cluster at his or her convenience.

However, this scenario only works when the user migrates a container cluster among major cloud providers including Google Cloud Platform (GCP), Amazon Web Applications (AWS), and Microsoft Azure.
This is because Kubernetes fails to completely hide differences in base environments.
Kubernetes does not provide generic ways to route the traffic from the internet into container cluster running in the Kubernetes and expects the base infrastructure automatically route traffic to nodes that might host container.
In other words, Kubernetes is heavily dependent on cloud load balancers, which is external load balancers that are set up on the fly by cloud providers through their application protocol interfaces (APIs).
Once the traffic reaches the nodes, Kubernetes handles it nicely, but this is a problem since not every cloud provider or on-premise data center has load balancers that can be set up through API and utilized by Kubernetes.
Other container cluster management systems, e.g. Docker swarm, etc, also lack a generic way to route the traffic into the container cluster.
Therefore this is one of the generic problems that current container cluster architectures possess.

Load balancers are often used to distribute high volume traffic from the Internet to thousands of web servers.
They are implemented as dedicated hardware or software on commodity hardware.
Major cloud providers have developed software load balancers\cite{eisenbud2016maglev,patel2013ananta} as a part of their infrastructures.
They claim that their load balancers have a high-performance level and scalability.
Those software load balancers have APIs through which an outside program can set up and control the behavior of the load balancers.
Once cloud load balancers are set up automatically and distribute incoming traffic to every server that hosts containers,
the traffic is then distributed again to destination containers using the iptables destination network address translation(DNAT)\cite{MartinA.Brown2017,Marmol2015} rules in a round-robin manner.

In the case of on-premise data centers, there are variety of proprietary hardware load balancers.
It is very likely that most of the load balancers are left unsupported by Kubernetes, even if some of the load balancers may have APIs through which a container management system can set up and control the behavior.
In these cases, the user needs to manually configure the static route for inbound traffic in an ad-hoc manner.
Since the Kubernetes fails to provide a uniform environment from a container cluster viewpoint, migrating container clusters among the different environments will always require daunting tasks.

\subsection{Contribution}

In order to achieve these aims, the author proposes a portable and scalable software load balancer that can be used in any environment including cloud providers and in on-premise data centers.
By using such a load balancer, users do not need to manually adjust their services to the base infrastructures.
As a proof of concept the author implements the proposed software load balancer that works well with with Kubernetes using following technologies;
1) To make the load balancer usable in any environment, Linux kernel's Internet Protocol Virtual Server (ipvs)\cite{Zhang2000} is containerized using Docker\cite{merkel2014docker}. 
2) To make the load balancer redundant and scalable, the author makes it capable of updating the routing table of upstream router with Equal Cost Multi-Path(ECMP) routes\cite{al2008scalable} using a standard protocol, Border Gateway Protocol(BGP).
3) The author also extends the research into implementing the novel load balancer using eXpress Data Plane(XDP) technology\cite{bertin2017xdp} to enhance the performance level to meet the need for 10Gbps network speed.

Contributions of this paper are as follows:
Although there have been studies regarding redundant software load balancers especially from the major cloud providers\cite{eisenbud2016maglev,patel2013ananta}, their load balancers are only usable within their respective cloud infrastructures.
Therefore in order to facilitate container cluster migrations, a software load balancer architecture with redundancy and scalability that is common to any base infrastructure has been needed.
This paper aims to provide such a load balancer architecture and evaluate a proof-of-concept system that is built using Open Source Software(OSS) technologies.
The understanding obtained from a detailed analysis of the evaluation also helps both the research community and the web application industry, because there does not exist enough of them.
Moreover, since proposed load balancer architecture uses nothing but existing OSSs and standard Linux boxes, users can build a cluster of redundant load balancers in their environment.

The outcome of this study will benefit users who want to deploy their web applications on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of our study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web application on aggregated computing resources on the earth, as if they were starting a single process on a single computer.

\section{Outline}

The rest of the paper is organized as follows.
Chapter \ref{chapter:background} provides the background information and related works.
Chapter \ref{chapter:architecture} provides the problems of existing load balancers and proposes suitable architectures.
Chapter \ref{chapter:implemetation} presents implementation of the proposed load balancer architecture in detail.
Chapter \ref{chapter:portablelb} discusses portability and performance levels of the proposed load balancer in 1 Gbps network environment.
Chapter \ref{chapter:redundancy} discusses the redundancy and scalability of the proposed load balancers.
Chapter \ref{chapter:performance} present the performance levels of the proposed load balancer in 10 Gbps network environment and discuss the method to improve the performance of a software load balancer.
Chapter \ref{chapter:futurework} discusses the limitation and the future work of this study,
which is followed by a conclusion of this work in Chapter \ref{Conclusions}.





