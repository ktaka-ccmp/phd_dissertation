%\section{Introduction}

\section{Motivation of the research}
Nowadays, a great number of people in the world can not spend a day without using smartphones or personal computers(PCs) to retrieve information for work or for dailylife from the internet.
For example, people use those devices to look up weather, news, emails, social media and sometimes to play games.
These services are often called web services, where information is deliverd using Hyper Text Transfer Protocols(HTTP) or Hypertext Transfer Protocol Secure (HTTPS) from servers at the other end of the internet.
Web services are provided by various organizations, including commercial companies, government, non-profitable organizations, schools, etc.
A client program on PCs or smartphone sends out requests to servers and the servers respond with data that is requested, using HTTP or HTTPS. 

Servers for web services are usually computers located in a data center.
Servers also refer to the server programs that are runing on those computers. 
Multiple servers cooperate to fulfill the need of the clients.
The author call a group of those servers a web cluster or a web service cluster.
Fig. xxx shows schematic diagram of an example of a web cluster.
There are several servers that work together to respond to requests from clients.
There are also load balancers that distribut requests to multiple web servers.
Those servers are often purchased by web service providers and located in server housing facilities called data centers.

The emergence of Cloud Computing made it easier for service providers to deploy web services than before.
Cloud providers utilize virtual machines(VM) where multiple VMs share a single physical server.
They charge their customers with finer granularities in pay-as-you-go bases.
From the point of view of cloude users (i.e. web service providers), this lowers initial cost spent on infrastructures for their services.
It also shortens the time to start a service from the inception of the project, hence gives the users agility.
And at the time when the computing resources for a service is excessive, users can easily decrease the VMs.

More recently, Linux containers\cite{menage2007adding} have come to draw a significant amount of attention because they are lightweight, portable, and reproducible.
Linux containers are generally more lightweight than virtual machines(VMs), because the containers share the kernel with the host operating system (OS), even though they maintain separate execution environments.
Linux containers can be run on top of Linux OS, therefore they can be run in most of the cloud infrastructures and on-premise data centers. 
They are generally portable because the process execution environments are archived into tar files,
so whenever one attempts to run a container, the exact same file systems are restored from the archives even when totally different data centers are used.
This means that containers can provide reproducible and portable execution environments.

For the same reasons, Linux containers are attractive for web services as well, and it is expected that web services consisting of a cluster of containers would be capable of being migrated easily for a variety of purposes.
For example disaster recovery, cost performance optimizations, meeting legal compliance and shortening the geographical distance to customers are the main concerns for web service providers in e-commerce, gaming, Financial technology(Fintech) and Internet of Things(IoT) field.

The motivation of this research is to develop tools and infrastructures that will enable users to deploy their services across the world seamlessly, regardless of the cloud providers or data centers they use.
Also, the author aims to realize the future where users can choose whatever infrastructure they like without sacrificing advanced features that are provided only by limited cloud providers.

\section{Lock-in problem}

It is desirable if users can migrate their services to multiple of cloud providers or on-premise data centers seamlessly, which spread across the world.
Container cluster management systems facilitate these usages by functioning as middlewares, which hide the differences among cloud providers and on-premise data centers.

Kubernetes\cite{K8s2017}, which is one of the most popular container cluster management systems, enables easy deployment of container clusters.
Kubernetes are initially developed by engineers inside Google, to facilitate container cluster deployment for web services.
Kubernetes allows users to deploy a cluster of containers each of which depends on each other, with ease of launching a single program.
It also allows users to increase or decrease the number of containers dynamically.

Since Kubernetes is expected to hide the differences in the base environments, it is expected that users can easily deploy a web service on different cloud providers or on on-premise data centers, without adjusting the container cluster configurations to the new environment. 
This allows a user to easily migrate a web service consisting of a container cluster even to the other side of the world with the following senario; 
A user starts the container cluster in the new location, route the traffic there, then stop the old container cluster at his or her convenience.
This is a typical web service migration scenario.

However, this scenario only works when the user migrates a container cluster among major cloud providers including Google Cloud Platform (GCP), Amazon Web Services (AWS), and Microsoft Azure.
This is because Kubernetes partially hides differences in base environments.
Kubernetes does not provide generic ways to route the traffic from the internet into container cluster running in the Kubernetes and expects the base infrastructure route traffic to every node that might host container.
In other words, Kubernetes is heavily dependent on cloud load balancers, which is external load balancers that are set up on the fly by cloud providers through their application protocol interfaces (APIs).
Once the traffic reaches the nodes, Kubernetes handles it nicely, but this is a problem since not every cloud provider or on-premise data center has load balancers capable of being utilized by Kubernetes.

Other container cluster management systems, e.g. docker swarm, etc, also lack a generic way to route the trafic into the container cluster.
One of the aims of this study is to seek a generic way to route the traffic into container clusters, and thereby facilitating web service migrations.

Load balancers are often used to distribute high volume traffic from the Internet to thousands of web servers .
Major cloud providers have developed software load balancers\cite{eisenbud2016maglev,patel2013ananta} as a part of their infrastructures.
They claim that their load balancers have a high-performance level and scalability.
Once cloud load balancers distribute incoming traffic to every server that hosts containers,
the traffic is then distributed again to destination containers using the iptables destination network address translation(DNAT)\cite{MartinA.Brown2017,Marmol2015} rules in a round-robin manner.

In the case of on-premise data centers, there are variety of proprietary hardware load balancers.
The actual implementation and the performance level of those existing load balancers are very different and are most likely not supported by Kubernetes.
In those environments, the user needs to manually configure the static route for inbound traffic in an ad-hoc manner.
Since the Kubernetes fails to provide a uniform environment from a container cluster viewpoint, migrating container clusters among the different environments will always require a daunting tasks.
The other aims of this study is to provide a load balancer that woks well with Kubernetes for environments lacking support by Kubernetes, and thereby facilitating web service migrations.

\section{Contribution}

In order to achieve these aims, the author proposes a portable and scalable software load balancer that can be used with Kubernetes in any environment including cloud providers and in on-premise data centers.
Users now do not need to manually adjust their services to the infrastructures.
As a proof of concept the author implements the proposed software load balancer using following technologies;
1) To make the load balancer usable in any environment, we containerize ipvs\cite{Zhang2000} using Linux container technology\cite{menage2007adding}.
2) To make the load balancer redundant and scalable, we make it capable of updating the routing table of upstream router with Equal Cost Multi-Path(ECMP) routes\cite{al2008scalable} using a standard protocol, Border Gateway Protocol(BGP).
In order to make the load balancer's performance level to meet the need for 10Gbps network speed, a software load balancer that better performs than ipvs is required.
The author also extends the research into implementing the novel load balancer using eXpress Data Plane(XDP) technology\cite{bertin2017xdp} to enhance the performance level.

The author implements containerized Linux kernel's Internet Protocol Virtual Server (ipvs)\cite{Zhang2000} Layer 4 load balancer using a Kubernetes ingress\cite{K8sIngress2017} framework, as a proof of concept.
Then extend it to support Equal Cost Multi-Path(ECMP)\cite{thaler2000multipath} redundancy by running a Border Gateway Protocol(BGP) agent container together with ipvs container.
Functionality and performances are evaluated for each of them.

Although major cloud providers do not currently provide BGP peering service for their users, the authors expect our proposed load balancer will be able to run, once this approach is proven to be beneficial and they start BGP peering services.
Therefore we focus our discussions on verifying that our proposed load balancer architecture is feasible, at least in on-premise data centers.
For the cloud environment without BGP peering service, single instance of ipvs load balancer can still be run with redundancy.
The liveness of the load balancer is constantly checked by one of the Kubernetes agents, and if anything that stop the load balancer happens, Kubernetes will restart the load balancer container.
The routing table of the cloud provider can be updated by newly started ipvs container immediately.

The authors limit the focus of this study on providing a portable load balancer for Kubernetes to prove the concept of proposed architecture.
However, the same concept can be easily applied to other container management systems, which should be discussed in future work.

The contributions of this paper are as follows:
Although there have been studies regarding redundant software load balancers especially from the major cloud providers\cite{eisenbud2016maglev,patel2013ananta}, their load balancers are only usable within their respective cloud infrastructures.
This paper aims to provide a redundant software load balancer architecture for those environments that do not have load balancers supported by Kuberenetes.
The understanding obtained from detailed analysis of the evaluation also helps both the research community and web service industry, because there is not enough of them.
Moreover, since proposed load balancer architecture uses nothing but existing Open Source Software(OSS) and standard Linux boxes, users can build a cluster of redundant load balancers in their environment.

The outcome of our study will benefit users who want to deploy their web services on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of our study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web service on aggregated computing resources on the earth, as if they were starting a single process on a single computer.

\section{Outline}

The rest of the paper is organized as follows.
Chapter \ref{chapter:background} provides the backgrounf information and related works.
Chapter \ref{chapter:architecture} provides the problems of existing load balancers and proposes suitable architectures.
Chapter \ref{chapter:implemetation} presents implementation of the proposed load balancer architecture in detail.
Chapter \ref{chapter:portablelb} discusses portability and performance levels of the proposed load balancer in 1 Gbps network environment.
Chapter \ref{chapter:redundancy} discusses the redundancy and scalability of the proposed load balancers.
Chapter \ref{chapter:performance} present the performance levels of the proposed load balancer in 10 Gbps network environment, and discuss the method to improve the performance of a soft ware load balancer.
Chapter \ref{chapter:futurework} discusses the limitation and the future work of this study, 
which is followed by a conclusion of this work in Chapter \ref{Conclusions}.





