%\section{Introduction}

\section{Motivation of the research}

\subsection{web app in data center}

\begin{itemize}
\item switching DC was hard 
\item expanding Service to other conuntries
\item prep disaster
\end{itemize}

\subsection{web in cloud}

\begin{itemize}
\item low cost
\item quick deployment
\end{itemize}

\subsection{web app in container}

\begin{itemize}
\item less over head
\item as quick as stating a process
\item container management system schedule containers
\end{itemize}

Another aspect of the container management system is interesting. 
It can be viewed as the Operating System for a cluster of servers, where it not only provides scheduling of proccesses but also route the traffic to the right processes.
In this way, a cluster of computers can be used to provide web services with the ease of using a single computer.


\subsection{Desireble infrastructure using container}

\begin{itemize}
\item Universal Container management system as a middle ware
\item Global data storage as Google spanner, Conckroach database
\item Global routing, Anycast 
\end{itemize}

The author focus on Load balancer.
Software load balancer that work well with container env.

\begin{itemize}
\item How is the performance?
\item How is that scable?
\end{itemize}

\subsection{Old}

Today, a great number of people in the world can not spend a day without using smartphones or personal computers(PCs) to retrieve information from the Internet for work or for daily life.
For example, people use these devices to look up web pages, emails, social media and sometimes to play games.
These services are often called web applications, where information is delivered using Hyper Text Transfer Protocols(HTTP) or Hypertext Transfer Protocol Secure (HTTPS) from servers at the other end of the Internet.
Web applications are provided by various organizations, including commercial companies, government, non-profitable organizations, schools, etc.
(The author calls them web application providers hereafter.)
A client program on PCs or smartphone sends out requests to servers and the servers respond with data that is requested, using HTTP or HTTPS. 

Servers for web applications are usually computers located in a data center.
%Servers also refer to the server programs that are runing on these computers. 
In the data center multiple servers cooperate to fulfill the need of the clients.
A group of these servers is often called a web application cluster or a web cluster.
Figure~\ref{fig:web_cluster} shows schematic diagram of an example of a web cluster.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\columnwidth]{Figs/web_cluster.png}
\end{center}
\caption{
An example of web cluster.
}
\label{fig:web_cluster}
\end{figure}

In this example, there are two load balancers, four web servers and two Database(DB) servers that work together to respond to requests from clients.
The load balancers distribute requests from clients to multiple web servers. 
Then the web servers form the response using the data retrieved from the database servers, and send it back to the client.
Sometimes the web servers may store and update important data into the database servers.

Web application providers often purchase these servers and locate them in server housing facilities called data centers.
In this type of infrastructure, the web application providers typically need to sign a contract with data center company for server housing spaces and buy servers and install them in their rented spaces in a data center by themselves.
Preparing data centers, installing the servers and configuring software stacks for their services often require considerable time and money.
If they want to expand their services to different countries or if they want to prepare for natural disasters by preparing an additional web cluster in a different data center, they most likely need about the same amount of money and hassles required to build their original infrastructures.
Since the servers are located in the users own facilities or rented facilities, and the users are responsible for managing servers, this type of infrastructure is called on-premise infrastructure.

The emergence of Cloud Computing made starting a new service easier for web application providers than before.
Cloud computing utilizes virtual machine(VM) technology, e.g. KVM, Xen and VMware, and VMs are rented by users(e.g. web application providers) pay-per-use basis.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\columnwidth]{Figs/physical_vm_container.png}
\end{center}
\caption{
Different n multi tenant example architecture of virtual machine technology. Virtual machines share a single physical server.
}
\label{fig:physical_vm_container}
\end{figure}

Figure~\ref{fig:physical_vm_container} (b) shows an example architecture of VM technology. 
VMs share a single physical server.
A full OS including Linux kernel is running in every VM, and each VM behaves almost as same as a single physical server.
Since VMs are fractions of a single physical server, server resources are utilized with finer granularities.
Users can start their services with a cluster of VMs, resulting in lower cost.
Cloud providers also prepare physical servers and software stacks for VMs before renting it to users.
As a result, users need only to click a few buttons on web consols, and then, up-and-running VMs are available for them.
This will bring agility to users when they launch their services.
And since computing resources are offered as a pay-par-uses basis with per-second billing, when the computing resources for a service is more than enough, users can quickly reduce cost by stopping excessive VMs.
This was impossible when they bought servers and installed them in the datacenter.
In short cloud computing brought us agility, flexibility, and cost-effectiveness.


More recently, Linux containers\cite{menage2007adding} have come to draw a significant amount of attention.
Figure~\ref{fig:physical_vm_container} (c) shows an example architecture of container technology. 
Container technology utilizes Linux kernel's namespace feature to separate processes.  
Every process is assigned to a certain namespace, and if two processes belong to different namespaces, they can not see each other's resources. 
Linux kernel implements filesystem, PID, network, user, IPC, and hostname namespaces. 
For example, each filesystem namespace has its own root filesystem, and each network namespace has its own network devices and IP addresses.
Therefore, it is possible to configure processes as if they were running in different Linux systems by assigning them to different namespaces, although they share kernel and hardware.
Whilst VMs needed to run a full OS on top of a hypervisor, thus imposing extra overhead, a process in Linux containers is as light as a single process.
%
Several management tools are available for Linux containers, including LXC\ref{}, systemd-nspawn\ref{}, lmctfy\ref{} and Docker\ref{}.
These tools restore file system from image (archive) file, launch processes and assign appropriate namespaces.
Due to the widespread usage of Linux systems, Linux containers run in most of the cloud infrastructures and on-premise data centers.
Linux containers are reproducible because the process execution environments are archived into tar files.
Whenever one attempts to run a container, the exact same file systems are restored from the archive even when totally different data centers or cloud providers are used.
Since Linux containers are generally more lightweight than virtual machines(VMs), and they are portable and reproducible, 
cloud providers are starting to offer services utilizing container technologies.

For these reasons, Linux containers are attractive for web applications as well, and it is expected that web applications consisting of a cluster of containers can be run anywhere regardless of the difference in base infrastructures, i.e. cloud providers or data centers.

migrated easily for a variety of purposes.
For example disaster recovery, cost performance optimizations, meeting legal compliance and shortening the geographical distance to customers are the main concerns for web application providers in e-commerce, gaming, Financial technology(Fintech) and Internet of Things(IoT) field.

The purpose of this research is to enable web application providers to easily deploy their services across the world seamlessly, regardless of cloud providers or data centers they use, by better-utilizing container cluster technology. 
Also, the author aims to realize the future where users can choose whatever infrastructure they like without sacrificing advanced features that are provided only by limited cloud providers.

\section{Avoid lock-in problem}

It is desirable if users can migrate their services to multiple of cloud providers or on-premise data centers seamlessly, which spread across the world.
Container cluster management systems facilitate these usages by functioning as middlewares, which hide the differences among cloud providers and on-premise data centers.

Kubernetes\cite{K8s2017}, which is one of the most popular container cluster management systems, enables easy deployment of container clusters.
Kubernetes are initially developed by engineers inside Google, to facilitate container cluster deployment for web applications.
Kubernetes allows users to deploy a cluster of containers each of which depends on each other, with the ease of launching a single application program.
It also allows users to increase or decrease the number of containers dynamically depending on the amount of traffic that they have to respond.

Since Kubernetes is expected to hide the differences in the base environments, it is expected that users can easily deploy a web application on different cloud providers or on on-premise data centers, without adjusting the container cluster configurations to the new environment. 
This allows a user to easily migrate a web application consisting of a container cluster even to the other side of the world.
A typical web application migration scenario is; 
a user starts the container cluster in the new location, route the traffic there, then stop the old container cluster at his or her convenience.

However, this scenario only works when the user migrates a container cluster among major cloud providers including Google Cloud Platform (GCP), Amazon Web Applications (AWS), and Microsoft Azure.
This is because Kubernetes fails to completely hide differences in base environments.
Kubernetes does not provide generic ways to route the traffic from the internet into container cluster running in the Kubernetes and expects the base infrastructure automatically route traffic to nodes that might host container.
In other words, Kubernetes is heavily dependent on cloud load balancers, which is external load balancers that are set up on the fly by cloud providers through their application protocol interfaces (APIs).
Once the traffic reaches the nodes, Kubernetes handles it nicely, but this is a problem since not every cloud provider or on-premise data center has load balancers that can be set up through API and utilized by Kubernetes.
Other container cluster management systems, e.g. Docker swarm, etc, also lack a generic way to route the traffic into the container cluster.
Therefore this is one of the generic problems that current container cluster architectures possess.

Load balancers are often used to distribute high volume traffic from the Internet to thousands of web servers.
They are implemented as dedicated hardware or software on commodity hardware.
Major cloud providers have developed software load balancers\cite{eisenbud2016maglev,patel2013ananta} as a part of their infrastructures.
They claim that their load balancers have a high-performance level and scalability.
Those software load balancers have APIs through which an outside program can set up and control the behavior of the load balancers.
Once cloud load balancers are set up automatically and distribute incoming traffic to every server that hosts containers,
the traffic is then distributed again to destination containers using the iptables destination network address translation(DNAT)\cite{MartinA.Brown2017,Marmol2015} rules in a round-robin manner.

In the case of on-premise data centers, there are variety of proprietary hardware load balancers.
It is very likely that most of the load balancers are left unsupported by Kubernetes, even if some of the load balancers may have APIs through which a container management system can set up and control the behavior.
In these cases, the user needs to manually configure the static route for inbound traffic in an ad-hoc manner.
Since the Kubernetes fails to provide a uniform environment from a container cluster viewpoint, migrating container clusters among the different environments will always require daunting tasks.
One of the aims of this study is to seek a generic way to route the traffic into container clusters automatically, by providing a software load balancer that works well with the container management systems, and thereby to facilitate web application migrations.

\section{Contribution}

In order to achieve these aims, the author proposes a portable and scalable software load balancer that can be used in any environment including cloud providers and in on-premise data centers.
By using such a load balancer, users do not need to manually adjust their services to the base infrastructures.
As a proof of concept the author implements the proposed software load balancer that works well with with Kubernetes using following technologies;
1) To make the load balancer usable in any environment, Linux kernel's Internet Protocol Virtual Server (ipvs)\cite{Zhang2000} is containerized using Docker\cite{merkel2014docker}. 
2) To make the load balancer redundant and scalable, the author makes it capable of updating the routing table of upstream router with Equal Cost Multi-Path(ECMP) routes\cite{al2008scalable} using a standard protocol, Border Gateway Protocol(BGP).
3) The author also extends the research into implementing the novel load balancer using eXpress Data Plane(XDP) technology\cite{bertin2017xdp} to enhance the performance level to meet the need for 10Gbps network speed.

Contributions of this paper are as follows:
Although there have been studies regarding redundant software load balancers especially from the major cloud providers\cite{eisenbud2016maglev,patel2013ananta}, their load balancers are only usable within their respective cloud infrastructures.
Therefore in order to facilitate container cluster migrations, a software load balancer architecture with redundancy and scalability that is common to any base infrastructure has been needed.
This paper aims to provide such a load balancer architecture and evaluate a proof-of-concept system that is built using Open Source Software(OSS) technologies.
The understanding obtained from a detailed analysis of the evaluation also helps both the research community and the web application industry, because there does not exist enough of them.
Moreover, since proposed load balancer architecture uses nothing but existing OSSs and standard Linux boxes, users can build a cluster of redundant load balancers in their environment.

The outcome of this study will benefit users who want to deploy their web applications on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of our study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web application on aggregated computing resources on the earth, as if they were starting a single process on a single computer.

\section{Outline}

The rest of the paper is organized as follows.
Chapter \ref{chapter:background} provides the background information and related works.
Chapter \ref{chapter:architecture} provides the problems of existing load balancers and proposes suitable architectures.
Chapter \ref{chapter:implemetation} presents implementation of the proposed load balancer architecture in detail.
Chapter \ref{chapter:portablelb} discusses portability and performance levels of the proposed load balancer in 1 Gbps network environment.
Chapter \ref{chapter:redundancy} discusses the redundancy and scalability of the proposed load balancers.
Chapter \ref{chapter:performance} present the performance levels of the proposed load balancer in 10 Gbps network environment and discuss the method to improve the performance of a software load balancer.
Chapter \ref{chapter:futurework} discusses the limitation and the future work of this study,
which is followed by a conclusion of this work in Chapter \ref{Conclusions}.





