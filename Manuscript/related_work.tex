
In this chapter, the author presents the related work of this study.
The purpose of this study has been to improve the portability of web applications by using container orchestrators as a common middleware.
Doing so will give users the freedom to migrate their services when there is a disaster, expand their businesses, and prevent vendor lock-ins, etc.
However, existing orchestrators fail to provide the same interfaces to web applications, because none of them has a standard method to fully automate the setup of routes for ingress traffic from the Internet.
Therefore, the author proposes a cluster of software load balancer containers for Kubernetes, which can be used in different infrastructures.

Following the logic, here the author presents related work regarding the following subjects:
(1) Portability of web applications.
(2) Container orchestrators.
(3) Software load balancers for Kubernetes.

Additionally, there are several software load balancers for cloud environments, which aims to differentiate their cloud infrastructure by seeking the best performances.
On the other hand, the load balancer proposed in this study has a different purpose, which is to provide a load balancer common to any infrastructure by using standard OSS technologies.
Despite the difference in purposes, it is worthwhile comparing the technology components in order to assess if the proposed load balancer is state of the art.
There are also miscellaneous techniques in implementing a load balancer in container, which should also be presented.

Therefore, the author also presents related work regarding the following subjects:
(4) Cloud load balancer.
(5) Load balancer tools in the container context.

\section{Portability of web applications}

\paragraph{\bf TOSCA:}

\paragraph{\bf Kubernetes federation:}

Kubernetes developers are trying to add federation \cite{K8sFederation2017} capability for handling situations 
where multiple Kubernetes clusters \footnote{The {\em Kubernetes cluster} refers to a server cluster 
controlled by the Kubernetes container management system, in this thesis.} 
are deployed on multiple cloud providers or on-premise data centers. 
Those Kubernetes clusters are managed by the Kubernetes federation API server (federation-apiserver).
Accoding to their explanation\cite{K8sFederation2017}, the federation capability provides the followings: 
\begin{quote}
High Availability: By spreading load across clusters and auto configuring DNS servers and load balancers, federation minimises the impact of cluster failure.
Avoiding provider lock-in: By making it easier to migrate applications across clusters, federation prevents cluster provider lock-in.
\end{quote}
The author regards the federation capability very attractive.
However, how each Kubernetes cluster is run on different types of cloud providers
and/or on-premise data centers, especially when the load balancers of such environments are not supported by Kubernetes, 
seems beyond the scope of that project.
This thesis is mainly focused on how to provide a common load balancers to different type of infrastructures. 

There have been numerous works regarding the portability of web applications.
This study is more focused on practical architecture and verification of its feasibility.

\section{Container orchestrators}

Docker/Swarm
Mesos/Marathon
Kubernetes

The swarm mode of the Docker\cite{DockerCoreEngineering2016,DockerInc2017} also uses ipvs for internal load balancing,
but it is also considered as part of Docker swarm infrastructure, 
and thus lacks the portability that our proposal aims to provide.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    & \multicolumn{1}{c|}{Kubernetes} & \multicolumn{1}{c|}{Docker Swarm} & \multicolumn{1}{c|}{Mesos Marathon} \\ \hline
    Config file & YAML & YAML & JSON  \\ \hline
    Scheduling & Yes &  Yes &  Yes  \\ \hline
    Ingress routing & \begin{tabular}{c} Manual$^{*}$ \\ Cloud load balancer$^{**}$\end{tabular} & Manual$^{*}$ & Manual$^{*}$ \\ \hline
    Internal routing & iptables DNAT & \replaced[id=2nd]{IPVS}{ipvs} &  haproxy  \\ \hline
  \end{tabular}
  \centering\parbox[c]{0.9\columnwidth}{
    \footnotesize{$^{*}$Users are expected to set up a static route to one of the internal load balancers manually. \par
    $^{**}$Support for Cloud load balancer is only available in limited infrastructures including GCP, AWS, Azure and OpenStack. }
  }
  \caption{Container orchestrator comparison.}
  \centering\parbox[c]{0.9\columnwidth}{
    Important aspects of features as web application infrastructures are compared.
  }
%  \label{table:orchestrator_comparison}
\end{table}

There are several container orchestrators that might be suited for portable web applications.
None of the existing orchestrators can fully automate the setup of routes for ingress traffic from the Internet, regardless of the base infrastructures.
At the moment Kubernetes seems best suited for web applications.

\section{Software load balancers for Kubernetes}

In the course of this study, other groups also have proposed ingress routing using IPVS for Kubernetes.
Here the author presents them.

First, Kubernetes comes with proxy daemon that setup internal load balancer on every node.
Until the author published the paper \cite{takahashi2018portable} regarding this thesis, the internal load balacer only used iptables DNAT.
Latest release of the Kubernetes offers the internal load balacer using IPVS.
Once the ingress traffic reaches one of the nodes, the packets are directed to existing {\em pods}.
In conventional setup, the traffic is manually routed to one of the nodes, which lacks the redundancy and scalability.
In cloud environments where there is supported load balancers, Kubernetes has a feature to automatically setup the cloud load balancer, so that the traffic is distributed all of the existing nodes. 

Nginx-ingress\cite{Pleshakov2016,NginxInc2016} utilizes the ingress\cite{K8sIngress2017} capability of Kubernetes,to implement a containerized Nginx proxy as a load balancer.
Nginx itself is famous as a high-performance web server program that also has the functionality of a Layer-7 load balancer.
Nginx is capable of handling Transport Layer Security(TLS) encryption, as well as Uniform Resource Identifier(URI) based switching.
However, the flip side of Nginx is that it is slower than Layer-4 switching.

The kube-keepalived-vip\cite{Prashanth2016} project is trying to use Linux kernel's ipvs\cite{Zhang2000} 
load balancer capabilities by containerizing the keepalived\cite{ACassen2016}.
The kernel ipvs function is set up in the host OS's net namespaces and is shared among multiple web services, as a part of the Kubernetes cluster infrastructure.
Our approach differs in that the ipvs rules are set up in container's net namespaces 
and function as a part of the web service container cluster itself.
The load balancers are configurable one by one, and are  movable with the cluster once the migration is needed.
The kube-keepalived-vip's approach lacks flexibility and portability whereas ours provide them.

MetalLB \cite{metallb} is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.
It has two operating modes, layer 2 mode, and BGP mode.
In the layer 2 mode, one of the nodes is chosen as a leader and the leader sends out gratuitous ARP (ipv4) or NDP (ipv6) packets to notify the upstream router.
The leader also responds to ARP and NDP requests.
In the BGP mode, each of the nodes establishes peering connection with the upstream router, announces themselves as a next hop of the service IP, and as a result, ECMP routing table can be created in the upstream router.
Once the ingress traffic reaches one of the nodes, the packets are directed to existing {\em pods} by the internal load balancer.
The problems with this implementation are as follows:
In the case of the layer 2 mode, failover is slow (more than about 10 secs) \cite{metallb}.
The ingress traffic is distributed to all of the nodes.
It is impossible to localize the routes to a limited number of the nodes.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    & \multicolumn{1}{c|}{OSS} & \begin{tabular}{c}Container \\friendly\end{tabular} & \multicolumn{1}{c|}{Redundancy} & \multicolumn{1}{c|}{Forwarding} & \multicolumn{1}{c|}{L3DSR} \\ \hline
    Conventional & No & No$^{*}$ & Static & iptables DNAT/IPVS & No \\ \hline
    Nginx-ingress & Yes & Yes & No & nginx & No  \\ \hline
    kube-keepalived & Yes & Yes & VRRP & IPVS & No  \\ \hline
    Metallb & Yes & Yes & ECMP$^{**}$ & IPVS & No  \\ \hline
    This work & Yes & Yes & ECMP & IPVS$^{***}$ & IPinIP  \\ \hline
  \end{tabular}
  }

  \par\bigskip
  \begin{minipage}{1.0\columnwidth}
    \caption[Comparison of software load balancers for Kubernetes]{
    Comparison of software load balancers for Kubernetes. 
  $^{*}$ Conventional technology uses cloud load balancers if available, which is not necessarily container friendly. 
  $^{**}$ Metallb also supports layer 2 mode, which uses unsolicited ARP or NDP packets to update layer 2 address table in the upstream router. 
  $^{***}$ The author plans to add XDP feature in future work.
    }   
    \label{tabl:k8s_lb}
  \end{minipage}

\end{table}

Table~\ref{tabl:k8s_lb} compares key features for above mentiond load balancers.
Regarding the redundancy, ECMP is better than VRRP because all the load balancer are active in the former case whereas only one of the load balancer is active in the latter.
As far as the L3DSR feature is concerned, the load balancer with this feature is beneficial because of the better performance.
The proposed load balancer is better than those in related works in these respects.

The proposed load balancer in this study differs in that it is deployed as part of a web application, giving the full control of the routing to the users rather than leaving them to the cluster administrators.
This will help resolve issues when there are routing problems.

\section{Cloud load balancers}

As far as the cloud load balancers are concerned, two articles have been identified.
Google's Maglev \cite{eisenbud2016maglev} is a software load balancer used in Google Cloud Platform(GCP).
Maglev uses modern technologies including per flow ECMP and kernel bypass for user space packet processing.
Maglev serves as the GCP's load balancer that is used by the Kubernetes.
Maglev is not a product that users can use outside of GCP nor is an open source software, while the users need open source software load balancer that can run even in on-premise data centers.

Microsoft's Ananta \cite{patel2013ananta} is another software load balancer implementation using ECMP and windows network stack.
Ananta can be solely used in Microsoft's Azure cloud infrastructure\cite{patel2013ananta}.
The proposed load balancer by the author is different in that it is aimed to be used in every cloud provider and on-premise data centers.

Facebook's Katran \cite{2018katran} is an OSS software load balancer using Linux XDP technology.
Katran can also be used in ECMP redundant setups.
Although Katran is expected to have high performance levels, nothing is published in the academic journals.

%% Although this work is not seeking the best software load balancer for cloud.
%% This work is not seeking the better performance and scalability.
%% This work is seeking to guarantee inter opearability while keeping decent performance.

%% The proposed load balancer in not yet competeing highest throughput.
%% The proposed load balancer in this thesis is more portable than cloud load balancers.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    & \multicolumn{1}{c|}{OSS} & \begin{tabular}{c}Container \\friendly\end{tabular} & \multicolumn{1}{c|}{Redundancy} & \multicolumn{1}{c|}{Forwarding} & \multicolumn{1}{c|}{L3DSR} \\ \hline
    Maglev & No & No & ECMP & Flexible I/O layer & GRE  \\ \hline
    Ananta & No & No & ECMP & Windows Filtering Platform  & IPinIP  \\ \hline
    Katran& Yes & No & ECMP & XDP & IPinIP  \\ \hline
    This work & Yes & Yes & ECMP & IPVS (XDP in future) & IPinIP  \\ \hline
  \end{tabular}
  }

  \par\bigskip
  \begin{minipage}{0.9\columnwidth}
    \caption[Cloud load balancer comparison]{
    Cloud load balancer comparison.
    }   
    \label{tabl:cloud_lb}
  \end{minipage}
\end{table}

Regarding the cloud load balancers, all of them try to differentiate their own cloud infrastructure by seeking the best performances.
On the other hand, this study attempts to provide a load balancer common to any infrastructure by using standard OSS technologies.
Despite the differences in purpose, the technology components used in this work and the related work are similar, which indicates that the proposed load balancer is state of the art.

\section{Load balancer tools in the container context}

There are several other projects where efforts have been made to utilize ipvs in the context of container environment.
For example, GORB\cite{Sibiryov2015} and clusterf\cite{Aaltodoc:http://urn.fi/URN:NBN:fi:aalto-201611025433} are daemons 
that setup ipvs rules in the kernel inside the Docker container. 
They utilize running container information stored in key-value storages
like Core OS etcd\cite{CoreOSEtcd} and HashiCorp's Consul\cite{HashiCorpConsul}. 
Although these were usable to implement a containerized load balancer in our proposal, we did not use them, 
since Kubernetes ingress framework already provided the methods to retrieve running container information through standard API.

\section{Summary}

In this chapter, the author presented the related work regarding the following subjects:
(1) Portability of web applications.
(2) Ingress routing in container orchestrators.
(3) Ingress routing in Kubernetes.
(4) Cloud load balancer.
(5) Load balancer tools in the container context.

There have been numerous works regarding the portability of web applications.
This study is more focused on practical architecture and verification of its feasibility.

There are several container orchestrators that might be suited for portable web applications.
None of the existing orchestrators can fully automate the setup of routes for ingress traffic from the Internet, regardless of the base infrastructures.
At the moment Kubernetes seems best suited for web applications.

In the course of this study, other groups also have proposed ingress routing using IPVS for Kubernetes.
Compared with those related works, the proposed load balancer in this study differs in that it is deployed as part of a web application.
Giving the full control of the routing to the users rather than leaving them to the cluster administrators will help resolve issues when there are problems.

Regarding the cloud load balancers, all of them try to differentiate their own cloud infrastructure by seeking the best performances.
On the other hand, this study attempts to provide a load balancer common to any infrastructure by using standard OSS technologies.
Despite the differences in purpose, the technology components used in this work and the related work are similar, which indicates that the proposed load balancer is state of the art.
