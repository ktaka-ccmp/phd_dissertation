\section{Conclusions}\label{Conclusions}


In this chapter, the author provided a discussion of load balancer architecture and its implementations suitable for container clusters.
%                                                                                                                                                            
First, the author discussed the problems of conventional architecture.
Since Kubernetes is dependent on external load balancers provided by the cloud infrastructures,
it failed to provide portability of a web application in environments where there was no supported load balancer.
Furthermore, the routes that ingress traffic from the internet follow were very complex and inefficient.
In order to alleviate these problems, the author proposed a cluster of software load balancers in containers.
The proposed load balancers utilized container technology and were managed by Kubernetes.
As a result, it is runnable on any environment including cloud infrastructures and on-premise data centers.
Furthermore, since Kubernetes manages load balancer containers, it can quickly scale the number of containers depending on the demand.
%                                                                                                                                                            
The author also discussed redundant architecture using ECMP with BGP for proposed load balancer containers.
By using the ECMP, the upstream router can route the ingress traffic to a cluster of load balancer containers in a redundant and scalable manner.
By using BGP, which is the standard protocol, ECMP routing rules in the upstream router are automatically populated, upon the launch of load balancer containers.
As a result users, i.e., web application providers can quickly and automatically set up routes to their web application, upon its launch.
This will greatly improve the portability of a web application and thereby enables migrations.


In this chapter, the performance levels of the proposed load balancer have been evaluated.
The author carried out throughput measurement to verify the feasibility of the load balancer and verified the followings;
From the throughput measurement of the proposed load balancer using physical servers in the on-premise data center, the author confirmed the followings;
The throughput of the proposed load balancer linearly increases as the number of nginx {\em pod}s increases, and then it eventually saturates, indicating the load balancer functions properly.
The throughput maximum depends on the multicore packet processing settings and overlay network settings.
The throughput of the ipvs in a container is equivalent to that of the iptables DNAT as a load balancer, in 1Gbps network environment.
The throughput of the ipvs container can be further improved up to 1.5 times by utilizing ipvs-tun mode, i.e., L3DSR.
The author also verified that the ipvs container was able to run and functioned properly, in both GCP and AWS.
While the performance levels of the proposed load balancers in cloud environments are inferior to those in on-premise data centers.
The ECMP routing update in the proposed architecture is properly functioning and quick enough.
The linear scalability of the ECMP throughput has been confirmed up to 4x of singel load balancer throughput.

In this chapter, the author carried out throughput measurements of ipvs, ipvs-tun, and iptables DNAT in 10Gbps environment.
From the results, the general characteristics of a load balancer are observesd.
The throughputs of ipvst and ipvs-tun are smaller than that of iptables DNAT in 10Gbps network, both due to the overhead of the container network and inefficiency in the program itself.
In order to improve the performance levels of portable load balancer, better network setup for containers and more efficient load balancer software should be developed.
However, considering the fact that the ultimate throughput of the system does not exceed that of the upstream router, the entrance,
the load balancers only need to be able to handle at most 293K, 2.9M, and 29M [req/sec], in 1Gbps, 10Gbps, and 100Gbps, respectively.
Since a single ipvs in container can handle 335K [req/sec], traffic equivalent to 2.9M [req/sec], which is the maximum throughput achievable in 10Gbps, can be easily handled by nine of ipvs containers.
The author also presented the priliminary result of xlb experiment, which will be needed in 100Gbps network environment.
The obtained per core throughput result and CPU usage result have been very promising.


\subsubsection{old}



In this dissertation, the author proposed a portable load balancer with ECMP redundancy for the Kubernetes cluster systems that is aimed at facilitating migration of container clusters for web services.
The proposed load balancer architecture utilizes software load balancers with container technology to make the load balancers runnable in any base infrastructure.
It also utilizes ECMP technology to make multiple load balancers active, and thereby to provide redundancy and scalability.

The author implemented a containerized software load balancer that is run by Kubernetes as a part of container cluster, using Linux kernel's IPVS.
In order to discuss the feasibility of the proposed load balancer, performance measurements are conducted in 1 Gbps network environment.
It was shown that the proposed load balancers are runnable in an on-premise data center, GCP and AWS.
Therefore the proposed load balancers can be said to be portable.
The throughput levels of a load balancer are dependent on settings for multi-core packet processing.
It was shown that better to use as many CPU cores as possible for packet processing.
The throughput levels are also very dependent on the overlay network backend mode.
The host-gw mode where no tunneling is used resulted in the best performance level, and the vxlan mode resulted in the second best.
In the experiment in 1 Gbps network environment, the ipvs-nat load balancer in the container had the same performance level as load balancing function of iptables DNAT on the node.
Furthermore, the performance level of ipvs-tun load balancer in a container with the L3DSR setup was about 1.5 times larger than that of iptables DNAT.
Therefore in 1 Gbps network environment, the proposed load balancer is portable while it has the 1.5 times better performance level or the same performance level depending on the mode of operation.

Also implemented is the ECMP setups where multiple of the load balancer containers are deployed, each advertising the route to the service VIP.
The ECMP technique makes the load balancers redundant and scalable since all the load balancer containers act as active.
The whole system is resilient to a single failure of load balancer container.
Also by utilizing multiple of load balancers simultaneously, the throughput of the total system is increased significantly.
These characteristics are evaluated by checking the routing table of the upstream router and by throughput measurement.
%
The author verified that ECMP routing table was properly created in the experimental system.
The update of the ECMP routing table was correct and quick enough, i.e., within 10 seconds, throughout 20 hours experiment.
The maximum performance levels of the cluster of load balancers scaled linearly as the number of the load balancer pods was increased up to four of them.
The maximum throughput level obtained through the experiment was 780k [req/sec], which is limited due to the maximum CPU performance of the benchmark client rather than the performance of the load balancer cluster.

The author also extended the throughput measurement into 10 Gbps network environment.
It was revealed that ipvs-nat and ipvs-tun load balancers in containers had lower performance levels compared with the iptables DNAT.
This has been suspected to be due to the overhead of the container network, i.e., veth+bridge.
By setting up the load balancing table in node net namespaces, the performance levels of ipvs-nat and ipvs-tun became closer to that of the iptables DNAT.
Although the overheads of the container network were invisible in 1 Gbps network environment, they were no longer invisible in 10 Gbps network environment.
The author is currently implementing and evaluating a novel software load balancer using XDP technology to provide a better alternative to ipvs as a portable load balancer.

The outcome of this study will benefit users who want to deploy their web services on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of this study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web service on aggregated computing resources on the earth, as if they were starting a single process on a single computer.




Network setup that does not degrade the performance is needed.
maybe ipvlan, macvlan
 
