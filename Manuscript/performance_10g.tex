\added[id=2nd]{In the previous chapter, the author evaluated the feasibility of the proposed load balancer in 1Gbps network.
The author verified that the proposed load balancer has sufficient throughput in 1 Gbps network.
In this chapter, the author shows that the proposed load balancer has sufficient throughput in 10Gbps network.
The author also discusses how to improve the performance levels in faster networks, e.g., 100 Gbps and finds that there are rooms for improvements in both the container network and the software load balancer itself.
Although these should be explored further in the future work, the author presents preliminary experimental results of a novel software load balancer using eXpress Data Plane (XDP) technology.
}


\section{Throughput measurement in 10G network}

In order to evaluate the performance levels of the proposed load balancer in a 10 Gbps network environment, the author carried out throughput measurements.
Table~\ref{tab:hw_sw_spec_10g} summarizes the hardware and software specification used in the experiment.
Bare metal servers with Intel X550 network card was used.
The X550 NIC has a maximum of 64 rx-queues, and 16 of them are activated by the driver at the boot time since there are 16 logical CPUs.
The setting \enquote{ (RSS, RPS)=(on, off)} is used because interrupts from each of 16 rx-queues can be assigned to separate logical cores.
As a result, packet processing is distributed to all of the 16 logical cores, which results in the best performance in most of the cases.
The host-gw mode is used as the backend mode of the flannel overlay network.

{
\setlength{\tabcolsep}{1em}
\renewcommand{\arraystretch}{1.2}

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \hline 
    \multicolumn{2}{l}{[Hardware Specification]}   \\
    & CPU: Xeon E5-2450 2.10GHz x 8 (with Hyper Threading) \\
    & Memory: 32GB \\
    & NIC: Intel X550 with 64 rx-queues (16 activated), 10 Gbps \\
    & (Node x 6, Load Balancer x 1, Client x 1)) \\
    & \\
    \multicolumn{2}{l}{[Node Software]}  \\
    & OS: Debian 9.5, linux-4.16.8 \\
    & Kubernetes v1.5.2 \\
    & flannel v0.7.0 \\
    & etcd version: 3.0.15 \\
    & \\
    \multicolumn{2}{l}{[Container Software]}   \\
    & Keepalived: v1.3.2 (12/03,2016) \\
    & nginx : 1.15.4 (web server) \\
  \hline 
  \end{tabular}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Hardware and software specifications for 10 Gbps experiment]{
      Hardware and software specifications for 10 Gbps experiment.
      There are 16 rx-queues activated for the NIC, to match the number of logical CPUs.
    }
    \label{tab:hw_sw_spec_10g}
  \end{minipage}
\end{table}
}

\begin{figure}[h]
  \begin{subfigure}[t]{\columnwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{Figs/benchmark-schem-10g-nat}
    \par\bigskip
    \centering
    \begin{minipage}{0.9\columnwidth}
      \caption{}
      \label{fig:benchmark-schem-10g-nat}
    \end{minipage}
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{Figs/benchmark-schem-10g-dsr}
    \par\bigskip
    \centering
    \begin{minipage}{0.9\columnwidth}
      \caption{}
      \label{fig:benchmark-schem-10g-dsr}
    \end{minipage}
  \end{subfigure}

  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Benchmark setups in 10 Gbps experiment]{
      Benchmark setups in 10 Gbps experiment.
      (a) The setup used in throughput measurements of \replaced[id=2nd]{IPVS}{ipvs} and iptables DNAT.
      The request and response packets both go through the load balancer node.
%      There is a bottleneck at the NIC of the load balancer node.
      (b) The setup used in throughput measurements of \replaced[id=2nd]{IPVS-TUN}{ipvs-tun}.
      The response packets for \replaced[id=2nd]{IPVS-TUN}{ipvs-tun}, return directly to the benchmark client.
%      The bottleneck is at the NIC of the benchmark client.
    }
    \label{fig:benchmark-schem-10g}
  \end{minipage}
\end{figure}

Figure~\ref{fig:benchmark-schem-10g} show experimental setups for the throughput measurements.
Multiple nginx {\em pods} are deployed on multiple nodes as web servers in the Kubernetes cluster.
In each nginx {\em pod}, single nginx web server program that returns the IP address of the {\em pod} itself is running.
The author then launched \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} pod as load balancers on one of the nodes, after that, the author performed the throughput measurement changing the number of the nginx web server pods.
On every Kubernetes node, there are iptables DNAT rules that function as an internal load balancer.
The author also measured throughput of the iptables DNAT as a load balancer.
The throughput is measured by sending out HTTP requests from the wrk towards a load balancer and by counting the number of responses the benchmark client received.
In the case of the \replaced[id=2nd]{IPVS-TUN}{ipvs-tun}, i.e., the tunneling mode of \replaced[id=2nd]{IPVS}{ipvs}, the response packets follow the different route than the case of conventional \replaced[id=2nd]{IPVS}{ipvs} and iptables DNAT.
As a result, the better performance level is expected for \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} since the load balancer node only has to deal with request packets of the traffic.

\FloatBarrier

Figure~\ref{fig:ipvs_l3dsr_10g} shows the throughput results of \replaced[id=2nd]{IPVS}{ipvs}, \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} and iptables DNAT in 10 Gbps environment.
The general characteristics of a load balancer, where the throughput increases linearly to a saturation level as the number of nginx container increases, can be seen.
The maximum throughput of each load balancer is limited by either packet forwarding efficiency of the software load balancer itself or the bandwidth of the network.
The maximum throughput level of the iptables DNAT is close to 780k [req/sec], where the CPU usage of the benchmark client was 100\%.
The maximum throughput levels of \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are less than that of iptables DNAT. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/ipvs_l3dsr_10g}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Throughput of load balancers in 10 Gbps]{
      Throughput of load balancers in 10 Gbps.
      The iptables DNAT rules exist in the node net namespace.
      The \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are in containers.
      The throughput of the iptables DNAT is the highest.
    }
    \label{fig:ipvs_l3dsr_10g}
  \end{minipage}
\end{figure}

Figure~\ref{fig:ipvs_l3dsr_10g} shows comparison of CPU usage between load balancers.
CPU usages are sampled on the load balancer nodes at the time of the throughput measurement using a program called dstat \cite{wieers2019dstat}.
It is seen that \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} uses less CPU resource than \replaced[id=2nd]{IPVS}{ipvs} because the load balancer node does not have to deal with the response packets.
The iptables DNAT uses even less CPU resource than \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun}.
Possible reasons for the lesser performance levels for \replaced[id=2nd]{IPVS}{ipvs} are as follows;
(1) It is possible that the \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} program themselves are less efficient than iptables DNAT.
(2) The network setup for the container, i.e., bridge+veth may be causing the overhead.
While iptables DNAT rules exist in node net namespace, proposed \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} exist in container net namespace.
In order to clarify which of these is the true reason for the performance difference, the author carried out throughput measurement for \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} without using the container network, i.e., in node net namespaces.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/cpu_usage_10g}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[CPU usage of load balancers in containers]{
      CPU usage of load balancers in containers.
      The iptables DNAT rules exist in the node net namespace.
      The \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are in containers.
      The iptables DNAT consumes the smallest amount of the CPU resource.
    }
    \label{fig:cpu_usage_10g}
  \end{minipage}
\end{figure}

\FloatBarrier

\subsubsection{Performance comparison in node net namespace}

The \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} load balancers were setup on one of the nodes. 
The load balancing rules were created in the node namespaces, and then throughput measurement were carried out.

Figure~\ref{fig:ipvs_l3dsr_10g_node_ns} shows the throughput of \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} in the node net namespace together with the throughput of the iptables DNAT.
The throughputs of the \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are improved from the previous results \added[id=2nd]{in Figure~\ref{fig:ipvs_l3dsr_10g}}.
\added[id=2nd]{This improvement indicates that the overhead due to container network using veth+bridge has a significant impact.}
%
The throughput of the \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} is almost identical to that of iptables DNAT.
\added[id=2nd]{
The maximum throughput of these are probably limited by the performance of the benchmark client since the CPU usages of the benchmark client at the saturation level were almost 100\% in both cases.
}

\deleted[id=2nd]{
Since the CPU usages of the benchmark client were almost 100\% at the saturated throughput for \replaced[id=2nd]{IPVS}{ipvs} and iptables DNAT, the actual maximum throughputs of these can be higher than this level.
On the other hand, the throughput of \replaced[id=2nd]{IPVS}{ipvs} is still less than those of \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} and iptables DNAT.
}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/ipvs_l3dsr_10g_node_ns}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Throughput of load balancers in node namespace]{
      Throughput of load balancers in node namespace.
      The performance levels of the \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are greatly improved from those in Figure~\ref{fig:ipvs_l3dsr_10g} by placing them in node net namespace.
    }
    \label{fig:ipvs_l3dsr_10g_node_ns}
  \end{minipage}
\end{figure}

Figure~\ref{fig:cpu_usage_10g_node_ns} shows CPU usages of each load balancers.
\added[id=2nd]{The CPU usage of the IPVS-TUN is smaller than that of IPVS.
This is because the load balancer does not process the response packets in L3DSR setting.
The CPU usage of the IPVS is still more extensive than that of iptables DNAT, indicating that IPVS is inherently less efficient than iptables DNAT.
}

\deleted[id=2nd]{
While the CPU usage of the \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} becomes less than that of iptables DNAT, the CPU usage of the \replaced[id=2nd]{IPVS}{ipvs} is still larger than that of iptables DNAT.
The author suspects that the \replaced[id=2nd]{IPVS}{ipvs} program itself is less efficient than iptables DNAT.
}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/cpu_usage_10g_node_ns}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[CPU usage of load balancers on nodes]{
      CPU usage of load balancers on nodes.
      CPU usages of \replaced[id=2nd]{IPVS}{ipvs} and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} greatly improved from those in Figure~\ref{fig:cpu_usage_10g} by placing them in node net namespace.  
      While the \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} consumes the smallest amount of the CPU resource, the CPU usage of \replaced[id=2nd]{IPVS}{ipvs} is still larger than that of iptables DNAT.
    }
    \label{fig:cpu_usage_10g_node_ns}
  \end{minipage}
\end{figure}

The author summarizes this section as follows;
The \replaced[id=2nd]{IPVS}{ipvs} itself is less efficient than iptables DNAT.
Using the container network, i.e., veth+bridge further degrades the throughput of \replaced[id=2nd]{IPVS}{ipvs}.
\added[id=2nd]{
  Therefore, both of these are the reasons for inferior performance levels for IPVS and IPVS-TUN in containers.
}

\FloatBarrier

\section{Discussion of required throughput}

The author has compared the performance of proposed load balancers in 10 Gbps in the previous section.
Although the proposed load balancer may not be the most efficient one, \added[id=2nd]{herein,} the author shows that it has \replaced[id=2nd]{sufficient}{enough} throughput for 10 Gbps network environment.

Table~\ref{tab:performance_summary} summarizes the maximum throughputs of the different load balancers obtained in the experiments so far.
Depending on the experimental conditions different part of the \added[id=2nd]{experimental} setup limits the throughput of the load balancers.
Since the performance bottlenecks are mainly due to network bandwidth in 1 Gbps network, it is easy to estimate the possible bottleneck due to bandwidth in a faster network.
Since benchmark client exists at the location where normally the upstream router exists, the performance bottleneck at the benchmark client corresponds to the maximum throughput that the load balancer should handle.

For example, the bottleneck at the benchmark client is about 293K [req/sec] in the 1 Gbps network. 
The load balancers only need to be able to handle at most 293K [req/sec] equivalent \added[id=2nd]{of the} traffic.
Even if the load balancer is capable of handling 500K [req/sec], the throughput of the system is ultimately determined by the bottleneck at the entrance, that is 293K [req/sec].
The maximum throughput in 1 Gbps, i.e., 293K [req/seq] is \replaced[id=2nd]{already}{easily} achieved by single \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} (293K [req/seq]) or two of \replaced[id=2nd]{IPVS}{ipvs} load balancers (193K x 2 [req/seq]) with ECMP redundancy.

In the case of 10 Gbps network, the maximum throughput determined by the bottleneck at the entrance is 2.9M [req/sec].
\added[id=2nd]{
  Single IPVS-TUN cannnot handle this much of the traffic.
}
\replaced[id=2nd]{However, t}{T}his is still \deleted[id=2nd]{easily} achievable \added[id=2nd]{without much of the hassle} by using four of \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} (731K x 4 [req/seq]) or nine of \replaced[id=2nd]{IPVS}{ipvs} load balancers (335K x 9 [req/seq]) with ECMP redundancy.

In the case of 100 Gbps network, where the load balancers are required to handle up to 29M [req/sec] of the throughput, the inefficiency of the software load balancer in a container can become a real problem.
Because in order to handle 29M [req/sec] of the traffic, 90 of \replaced[id=2nd]{IPVS}{ipvs} load balancer would be needed.
\added[id=2nd]{In this case, more efficient load balancer or container network is needed.}
In the next section, the author tries to improve the efficiency of a load balancer, by implementing novel XDP load balancer.
Except for such cases, the proposed load balancer can provide \replaced[id=2nd]{sufficient}{enough} throughput.

\begin{table}[h]

  \begin{subtable}{1\textwidth}
    \centering
    \begin{tabular}{|l|l|c|l|}
      \hline
      \multicolumn{1}{|c|}{Type} & \multicolumn{1}{c|}{namespace} &
      \begin{tabular}{c} Throughput \\ {[}req/sec{]} \end{tabular} & \multicolumn{1}{c|}{Bottleneck} \\ \hline
      iptables DNAT & node      & 193K &
      \begin{tabular}{l} Bandwidth filled with request \\ + response @ load balancer \end{tabular} \\ \hline
      IPVS          & container & 197K &
      \begin{tabular}{l} Bandwidth filled with request \\ + response @ load balancer \end{tabular} \\ \hline
      IPVS-TUN      & container & 293K &
      \begin{tabular}{l} Bandwidth filled with response \\ @ benchmark client  \end{tabular} \\ \hline
    \end{tabular}
    \caption{1 Gbps experiment}
  \end{subtable}

  \par\bigskip

  \begin{subtable}{1\textwidth}
    \centering
    \begin{tabular}{|l|l|c|l|}
      \hline
      \multicolumn{1}{|c|}{Type} & \multicolumn{1}{c|}{namespace} &
      \begin{tabular}{c} Throughput \\ {[}req/sec{]} \end{tabular} & \multicolumn{1}{c|}{Bottleneck} \\ \hline
      iptables DNAT & node      & 778K & \begin{tabular}{l} CPU\(\sim\)100\% @ benchmark client\end{tabular} \\ \hline
      IPVS          & container & 335K & \begin{tabular}{l} CPU\(\sim\)100\% @ load balancer node\end{tabular} \\ \hline
      IPVS-TUN      & container & 731K & \begin{tabular}{l} CPU\(\sim\)100\% @ load balancer node\end{tabular} \\ \hline
      IPVS          & node      & 700K & \begin{tabular}{l} CPU\(\sim\)100\% @ load balancer node\end{tabular} \\ \hline
      IPVS-TUN      & node      & 780K & \begin{tabular}{l} CPU\(\sim\)100\% @ benchmark client\end{tabular} \\ \hline
   \end{tabular}
      \caption{10 Gbps experiment}
  \end{subtable}

  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Summary of the maximum throughputs]{
      Summary of the maximum throughputs.
    }
    \label{tab:performance_summary}
  \end{minipage}
\end{table}

\FloatBarrier

\section{XDP load balancer}

The eXpress Data Path (XDP) \cite{hoiland2018express} is Linux kernel technology recently developed, where the tools and functionality to intercept and process the packets in the earliest phase as possible are provided.
By using XDP, one can write packet manipulation code using a subset of the C programing language, byte-compile it, and hook it into the place before the socket buffer is assigned, thereby speeding up network manipulation.
Typical applications include packet drop against DDOS attack, simple packet forwarding, and load balancing.
One of the benefits of the XDP compared to \replaced[id=2nd]{the technology using Data Plane Development Kit (DPDK) \cite{dpdkorg}}{DPDK} is that in the case of XDP, the packets that do not match the rule for processing are then passed to normal Linux kernel's network stack.
Therefore there is no need for preparing dedicated NIC for fast and efficient network processing.
The author implemented the XDP load balancer and carried out throughput measurement.

\subsubsection{Implementation}

Figure~\ref{fig:xlb-schem} show schematic diagram of the XDP load balancer, xlb, which is implemented by the author.
The xlb consists of programs and configurations shown in Table~\ref{table:xlb-progs}.
The xlb\_kernel.ll is the byte-compiled eBPF program that actually process the packets based on the load balance rules in the xlb table.
Load balancing rules are populated by a userspace daemon xlbd from the configration file xlbd.conf and nexthop and MAC address information obtained from the Linux kernel.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\columnwidth]{Figs/xlb-schem}

  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Xlb architecture]{
      Xlb architecture.
      The author implemented an XDP load balancer named xlb.
    }
    \label{fig:xlb-schem}
  \end{minipage}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Name & Function \\ \hline
    xlb\_kern.ll & byte compiled eBPF program  \\ \hline
    xlb & utility to inject eBPF program into kernl  \\ \hline
    xlbd & daemon to control load balance table  \\ \hline
    xlbd.conf & load balancing configuration  \\ \hline
    xlb table & load balance table  \\ \hline
    xlb cache & load balance cache  \\ \hline
  \end{tabular}

  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Xlb components]{
      Xlb components.
    }
    \label{table:xlb-progs}
  \end{minipage}
\end{table}

%% \subsubsection{Benchmark setup}

\subsubsection{Throughput results }

The author carried out the throughput measurement for the XDP load balancer, xlb.
The hardware and software configurations are same as the ones in Table~\ref{tab:hw_sw_spec_10g}.
The experimental setup is also same as the one in Figure~\ref{fig:benchmark-schem-10g}(b).
Since current implementation of xlb does not support multi core packet processing, the setting \enquote{(RSS,RPS)=(off,off)} is used in the throughput measurement.
All the interrupt from the NIC are notified to a single core.

Figure~\ref{fig:cpu_usage_10g_xlb} compares the throughput of xlb and iptables DNAT. 
Although a single core is used for the packet processing, the throughput of the xlb load balancer is 390k[req/sec], which is close to half of the iptables DNAT's throughput with 16 core (eight physical cores) packet processing.
Figure\ref{fig:cpu_usage_10g_xlb} compares CPU usages between xlb and iptables DNAT.
At a given throughput the xlb consumes much less CPU resource than iptables DNAT.
These results indicate that load balancer using XDP technology is very promising.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/xlb_iptables_dnat_10g}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[Throughput of xlb load balancer]{
      Throughput of xlb load balancer.
      The xlb load balancer is placed in node net namespace.
      The setting \enquote{(RSS,RPS)=(off,off)}, i.e., single core packet processing is used for the xlb measurement.
      The results of iptables DNAT for \enquote{(RSS,RPS)=(on,off)} and \enquote{(RSS,RPS)=(off,off)} are also shown for comparison.
      The throughput of the xlb is much higher than that of iptables DNAT with single core packet processing.
      Although using only a single core, the throughput of the xlb load balancer is close to half of the iptables DNAT's with 16 core (eight physical core) packet processing.
    }
    \label{fig:xlb_iptables_dnat_10g}
  \end{minipage}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{Figs/cpu_usage_10g_xlb}
  \par\bigskip
  \centering
  \begin{minipage}{0.9\columnwidth}
    \caption[CPU usage of xlb load balancer]{
      CPU usage of xlb load balancer.
      The xlb uses much less CPU resource than iptables DNAT.
    }
    \label{fig:cpu_usage_10g_xlb}
  \end{minipage}
\end{figure}

\FloatBarrier

\section{Summary}

\added[id=2nd]{
In this chapter, the author has shown that the proposed load balancer has sufficient throughput in 10 Gbps network.
The maximum throughput for IPVS-TUN is 731K [req/sec], which is sufficient because four of the load balancers with ECMP setup can deal with maximum traffic of 2.9M [req/sec] in 10 Gbps network.
}

\added[id=2nd]{
The author also discussed how to improve the performance levels for faster networks.
The throughputs of IPVS and IPVS-TUN in node net namespace are significantly improved from those in container net namespace.
This indicates that there is unignorable overhead due to container network using veth+bridge.
The CPU usage of the IPVS in node net namespace is more extensive than that of iptables DNAT, indicating that IPVS is inherently less efficient than iptables DNAT.
The author considers that both of these should be improved in future work.
}

\added[id=2nd]{
As an effort to improve the efficiency of the software load balancer, 
}
\replaced[id=2nd]{t}{T}he author has implemented a software load balancer, xlb, using XDP technology, and presented the preliminary experimental result.
The obtained throughput 390K [req/sec] with single core packet processing is very promising.
The author estimates that about five of the software load balancer using this technology with 16 core packet processing can provide \replaced[id=2nd]{sufficient}{enough} throughput, 29M [req/sec] in 100 Gbps environments in the future. 


\deleted[id=2nd]{
In this chapter, the author carried out throughput measurements of ipvs, \replaced[id=2nd]{IPVS-TUN}{ipvs-tun}, and iptables DNAT in 10Gbps environment.
The throughputs of ipvs and \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} are smaller than that of iptables DNAT in 10Gbps network, both due to the overhead of the container network and inefficiency in the program itself.
\added[id=2nd]{Therefore, there is a room for improvement in the actual implementation of portable load balancer in a container.}
\replaced[id=2nd]{However, considering}{Considering} the fact that the throughput of the whole system never exceeds that of the upstream router at the entrance, \added[id=2nd]{and} the load balancers only need to be able to handle at most 2.9M [req/sec] in 10Gbps network\replaced[id=2nd]{,}{.}
\replaced[id=2nd]{this}{This} can be \deleted[id=2nd]{easily }achievable using four of the \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} (L3DSR) load balancer container since a single \replaced[id=2nd]{IPVS-TUN}{ipvs-tun} in a container can handle 731K [req/sec].
Therefore the author considers the proposed load balancer \added[id=2nd]{using IPVS-TUN} has \replaced[id=2nd]{sufficient}{enough} performance for 10 Gbps network environment.
}

\deleted[id=2nd]{
For a faster network, improving the throughput of portable load balancer itself by using better network setup for containers and implementing more efficient software load balancer\added[id=2nd]{ instead of just using IPVS-TUN in containers}, becomes important.
The author has implemented a software load balancer, xlb, using XDP technology, and presented the preliminary experimental result.
The obtained throughput 390K [req/sec] with single core packet processing is very promising.
The author estimates that about five of the software load balancer using this technology with 16 core packet processing can provide \replaced[id=2nd]{sufficient}{enough} throughput, 29M [req/sec] in 100 Gbps environments in the future. 
}

\added[id=2nd]{
From these results, the author concludes this chapter as follows;
1) The proposed load balancer has sufficient performance levels in 10 Gbps network environment.
2) For a faster network, e.g., 100 Gbps, improvement in container network and in software load balancer itself is needed, which should be explored in future work.
3) The author implemented a software load balancer using XDP technology, and the preliminary result has shown that this technology is very promising.
}
