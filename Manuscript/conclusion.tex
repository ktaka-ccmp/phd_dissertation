\section{Conclusions}\label{Conclusions}

As the web services become an indispensable part of the daily life, portability of the application becomes very important.
%
\added[id=2nd]{In order to improve the portability of web applications consisting of container clusters, container orchestrators need to be able to serve as a uniform platform by functioning as a common middleware.}
\deleted[id=2nd]{Container orchestrators are expected to provide a uniform platform for container clusters and facilitate the migration of web applications consisting of container clusters.}
However, \added[id=2nd]{they fail to do so, because }none of the existing container orchestrators \added[id=2nd]{can fully automate the}\deleted[id=2nd]{ fully supports an automatic} setup of \added[id=2nd]{routes for} ingress traffic\deleted[id=2nd]{ routing} from the Internet\deleted[id=2nd]{, failing to serve as a uniform platform}.


\added[id=2nd]{
To solve this problem, the author proposed an architecture using a portable software load balancer that can run on any infrastructure.
The author proposed a cluster of software load balancers in containers that can be launched as a part of web applications for Kubernetes.
}
%
\added[id=2nd]{The proposed architecture is also capable of setting up the routes for the ingress traffic automatically in a redundant and scalable manner.
  For that purpose, Equal Cost Multi Path(ECMP) routes are populated through Border Gateway Protocol(BGP).
  Since both ECMP and BGP are the standard protocols, they are very likely to be supported by most of the upstream routers.
  By using the proposed architecture, container clusters no longer depend on the load balancers provided by infrastructures.
  And hence, container orchestrators become being able to better serve as a common middleware, which will improve the portability of the web applications consisting of container clusters.
}

\deleted[id=2nd]{In this dissertation, the author addresses this problem by proposing a portable software load balancer that is runnable on any infrastructure and capable of an automatic setup of the ingress traffic routing.
The proposed load balancer architecture utilizes software load balancers with container technology to make the load balancers runnable in any base infrastructure.
It also utilizes ECMP technology to make multiple load balancers active, and thereby to provide redundancy and scalability.
The proposed load balancer standardize the way to route traffic into container clusters, by removing dependencies on load balancers provided by infrastructures.
}

To prove the feasibility of the proposed load balancer architecture, the author has implemented a containerized software load balancer using Linux kernel's \replaced[id=2nd]{IPVS}{ipvs} for Kubernetes\deleted[id=2nd]{ and carried out performance measurements in the 1 Gbps network environment.}\added[id=2nd]{, and carried out experiments with the following criteria:}
\added[id=2nd]{
  1) verify if the proposed load balancer works correctly both in the cloud and the on-premise datacenter.
  2) verify if the proposed load balancer has a sufficient performance level for 1 Gbps and 10 Gbps networks.
  3) verify if the proposed redundancy architecture using ECMP with BGP properly functions.
}

\added[id=2nd]{
From the results of the experiments, it has been shown that the 
}
\deleted[id=2nd]{ The }throughput of the proposed load balancer linearly increases as the number of nginx {\em pod}s increases, and then it eventually saturates, indicating \added[id=2nd]{that} the load balancer functions properly.
It has been \added[id=2nd]{also} shown that the proposed load balancers \replaced[id=2nd]{can run}{are runnable} in an on-premise data center, Google Cloud Platform (GCP) and Amazon Web Service (AWS).
Therefore the proposed load balancers can be said to be portable.

The throughputs of a load balancer are dependent on the settings for multi-core packet processing and the setting for the overlay network.
\added[id=2nd]{To derive the best performance, the author used as many CPU cores as possible for packet processing, and the settings without any packet encapsulation for backend mode of the overlay network.}
\deleted[id=2nd]{It has been shown that the setting with as many CPU cores as possible for packet processing results in better performance.
It has been also shown that the backend mode for the overlay network without any packet encapsulation should be used for the best performance.}
%
\added[id=2nd]{From the experiment in the 1 Gbps network environment, the author obtained the highest throughput for the IPVS-TUN (L3DSR) in a container, which is limited by the bandwidth of the benchmark client.
  Since the benchmark client is placed at the same location where the upstream router exists, the load balancer can be said to have sufficient performance to fill up 1 Gbps network bandwidth.
}
\deleted[id=2nd]{The throughput of the ipvs in a container is equivalent to that of the iptables DNAT as a load balancer, in 1Gbps network environment.
The throughput of the ipvs in a container with Layer 3 Direct Server Return (L3DSR) setting has been about 1.5 times higher than that of existing iptables DNAT rules, which is prepared by Kubernetes's daemons as an internal load balancer.}

\added[id=2nd]{
The author also extended the throughput measurement into the 10 Gbps network environment, in order to verify that proposed software load balancer is capable of providing needed throughput for 10 Gbps environment.
The throughputs of IPVS and IPVS-TUN are smaller than that of iptables DNAT in 10Gbps network, both due to the overhead of the container network and inefficiency in the program itself.
Considering the fact that the throughput of the whole system never exceeds that of the upstream router at the entrance, the load balancers only need to be able to handle at most 2.9M [req/sec] in 10Gbps network.
This can be easily achieved using four of the IPVS-TUN (L3DSR) load balancer container since a single IPVS-TUN in a container can handle 731K [req/sec].
Therefore the author also concludes that although there is a room for improvements the proposed load balancer has sufficient performance for 10 Gbps network environment.
}

The author has also implemented an automatic setup of the ECMP route for ingress traffic.
There, multiple load balancer containers are deployed, and each of them advertises itself as an active next hop of the IP for web application through Border Gateway Protocol (BGP).
The ECMP route makes the load balancers redundant and scalable since all the load balancer containers act as active.
%
The BGP helps automatic setup of the ECMP route.  
The BGP and ECMP are both standard protocols supported by most of the commercial router products.
%
The author verified through experiment that an ECMP route has been automatically created upon launch of a new load balancer container on the upstream router.
The update of the ECMP routing table was correct and quick enough, i.e., within 10 seconds, throughout 20 hours experiment.
The maximum performance levels of the cluster of load balancers have scaled linearly up to four times as the number of the load balancer containers has been increased to four of them.
The maximum aggregated throughput obtained through the experiment is 780k [req/sec], which is limited by the CPU performance of the benchmark client, and therefore can be improved using better hardware in the future experiment.
Therefore the author has proved that proposed load balancer has the capability of the automatic setup of ingress traffic in a redundant and scalable manner.
%
\deleted[id=2nd]{
From these results, the author concludes that the proposed load balancer is portable, redundant and scalable while providing 1.5 times better throughput than iptables DNAT in 1Gbps network environment.
}

\deleted[id=2nd]{
The author also extended the throughput measurement into the 10 Gbps network environment, in order to verify that proposed software load balancer is capable of providing needed throughput for 10 Gbps environment.
The throughputs of ipvs and ipvs-tun are smaller than that of iptables DNAT in 10Gbps network, both due to the overhead of the container network and inefficiency in the program itself.
Considering the fact that the throughput of the whole system never exceeds that of the upstream router at the entrance, the load balancers only need to be able to handle at most 2.9M [req/sec] in 10Gbps network.
This can be easily achieved using four of the ipvs-tun (L3DSR) load balancer container since a single ipvs-tun in a container can handle 731K [req/sec].
Therefore the author also concludes the proposed load balancer has enough performance for 10 Gbps network environment.
}

\added[id=2nd]{Sooner or later, the day when the network in a data center becomes all 100 Gbps will come.}
\deleted{Sooner or later, the day will come when the network in a data center will become all 100 Gbps.
    }
\added{Therefore, in the future,}\deleted{For a faster network,} it \replaced{becomes crucial}{is important} to improve the throughput of portable load balancers by using better container network and implementing more efficient software load balancer itself.
The author leaves these for future work, however, a preliminary result of \added{the latter}\deleted{one of these} has also been presented.
The author has implemented a software load balancer using XDP technology and carried out throughput measurement.
The current implementation does not support multicore packet processing, and hence throughput is limited by the capability of single core processing performance.
\replaced[id=2nd]{Nevertheless}{However}, the obtained throughput about 390K [req/sec] for the XDP load balancer \added[id=2nd]{indicates that this technology} is very promising.
The author estimates that about five of the software load balancer using this technology with 16 core packet processing can provide enough throughput, 29M [req/sec] in 100 Gbps environments in the future. 

The proposed load balancer has been verified to be portable while providing \replaced[id=2nd]{sufficient}{enough} throughput in 10 Gbps environment.
\added[id=2nd]{
  And the proposed redundancy architecture using ECMP with BGP has also been verified to function properly.
  As a consequence, the proposed architecture with this load balancer will help improve the portability of web applications.
}

The outcome of this study will benefit users who want to deploy their web services on any cloud provider where no scalable load balancer is provided, to achieve high scalability.
Moreover, the result of this study will potentially benefit users who want to use a group of different cloud providers and on-premise data centers across the globe seamlessly.
In other words, users will become being able to deploy a complex web service on aggregated computing resources on the earth, as if they were starting a single process on a single computer.



 
